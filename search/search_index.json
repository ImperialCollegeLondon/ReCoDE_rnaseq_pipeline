{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RNA-seq Analysis Author: Jack Gisby Description The RNA-seq analysis exemplar involves the development of a pipeline for processing large volumes of biological data. The development of next generation sequencing technologies has facilitated a systems-level approach to biological and biomedical research. In particular, RNA sequencing (RNA-seq) has become a ubiquitous method for gene expression profiling. However, the colossal datasets produced by this method pose a new challenge for life science researchers, who commonly have little statistical or computational training. The processing of sequencing data commonly requires the development of custom workflows that can run in parallel on university computing clusters. Furthermore, the quality control and statistical analysis of these data requires specialist knowledge and purpose-built software packages. This project demonstrates the development of a pipeline for processing RNA-seq datasets on the computing clusters, such as the Imperial College Research Computing Service (RCS), and basic statistical analysis of the normalised data. This will involve: Quality control and trimming of raw RNA-seq reads Alignment of reads to the human reference genome Conversion of aligned reads to a matrix of gene counts Downstream statistical analysis: Data normalisation Unsupervised analysis (e.g. PCA) Differential expression and enrichment using edgeR Requirements Academic Familiarity with bash (A course such as \" The Linux Command Line for Scientific Computing \", hosted by the Imperial Research Computing & Data Science Team, would be provide a suitable background.) Familiarity with R programming language Familiarity with computing clusters System Program Version R Any Anaconda >=4.13.0 Access to Imperial's HPC Last updated: 29-07-2022 Learning Outcomes Upon completion of this tutorial, students will be able to: parallelise bioinformatics tools on computing clusters, such as the Imperial RCS develop a reproducible pipeline Tools used achieve this as part of the exemplar include: Nextflow , a workflow management system that makes it easy to develop data-driven pipelines. Conda , an package management system that allows you to share your local environment with others. Docker , an application for packaging dependencies into a virtual container. Git/GitHub , a version control system that integrates with nextflow to make pipelines shareable. Continous integration , a practice of automatic code testing. Getting started To get started using the pipeline, install Nextflow and either of Docker , Singularity or Conda . Then, the following command can be used to run the pipeline locally using a small test dataset: nextflow run -r main ImperialCollegeLondon/ReCoDE_rnaseq_pipeline -profile test,docker Note that this pipeline is not designed to handle all types of RNA-seq data (e.g. it was not designed for paired-read data). If you have large amounts of RNA-seq data to process, we recommend using the nextflow-core RNA-seq pipeline . To download the pipelines, install Git and clone this repository from the command line, using the following code: git clone https://github.com/ImperialCollegeLondon/ReCoDE_rnaseq_pipeline.git To run the first two pipelines, you will need to install the command line applications listed in environment.yml . Alternatively, you can use conda to install these applications, using the following code: conda env create -f environment.yml conda activate recode_rnaseq This conda environment also contains Nextflow, which we will be using to create the final iteration of the data processing pipeline. To run the nextflow pipeline, you will also need either Docker or Conda installed. The scripts in this repository are designed for use with the Imperial computing cluster. If you do not have access to the cluster, you might be able to adapt the code to your own cluster's configuration. Alternatively, the primary pipeline uses nextflow, which is adaptable to many different platforms. You could run the nextflow pipeline on your local computer, or configure it to run on another cluster or even the cloud. Some of these tools are not available on windows. If you wish to run the basic pipeline discussed in this document but you use Windows, you could use the Windows Subsystem for Linux for the course or work on a computing cluster, such as Imperial's high performance computing cluster . The downstream analysis steps for RNA-seq data require less compute power and often need a more customised workflow. So, this is demonstrated separately in an R markdown notebook. You can run this notebook, located at notebooks/downstream_analysis.Rmd , or you can view a complete markdown version of the notebook in docs/downstream_analysis.md . Project Structure In this exemplar, we set up three versions of the same pipeline, that process RNA sequencing data. They are available in docs , along with the background information necessary to follow each pipeline. Each of these pipeline generates .count files in their final step, which counts how many RNA sequences map to each gene for each sample. We created multiple versions of the pipeline to show the different ways in which researchers might want to process their data. They are as follows: simple_pipeline : This is the simplest version of the pipeline. It is a bash script that can be run straight from the command line. It will execute each stage of the pipeline in sequentially. Pros: Simple to follow and execute Cons: slow for a large number of samples if there is an error for a single sample the entire pipeline will fail parallelised_pipeline : This builds on the simple pipeline to run the pipeline on the cluster. It still uses bash scripts to orchestrate the pipeline, however it also allows each sample to be run in parallel on the cluster. This version of the pipeline is therefore a lot faster! Pros: a lot faster than simple_pipeline Cons: unwieldy to use for large numbers of samples nextflow_pipeline : This is the most advanced version of the pipeline. Instead of relying on bash scripts, that could fail without warning, it uses the nextflow workflow manager to run the pipeline stages either on your local computer or on a computing cluster. Pros: automatically orchestrate the running of large numbers of samples run seamlessly both locally, on computing clusters and on cloud platforms easily shared with others Cons: takes more time to set up than the simple or parallelised pipeline We created markdown documents, available in this directory, for each of these pipelines. These documents explain the pipelines and link to external resources in case you want to learn more. We suggest you go through the documentation in the order above, as each one builds upon the last. The downstream analysis uses the output of these pipelines to investigate the data. The dataset is a lot smaller at this point, so this stage of the analysis can be accomplished locally without the use of the cluster. The code for this stage is contained within the notebooks/ directory. The code is written in an Rmarkdown document, which has been run and stored as a markdown document within this directory.","title":"Home"},{"location":"#rna-seq-analysis","text":"","title":"RNA-seq Analysis"},{"location":"#author-jack-gisby","text":"","title":"Author: Jack Gisby"},{"location":"#description","text":"The RNA-seq analysis exemplar involves the development of a pipeline for processing large volumes of biological data. The development of next generation sequencing technologies has facilitated a systems-level approach to biological and biomedical research. In particular, RNA sequencing (RNA-seq) has become a ubiquitous method for gene expression profiling. However, the colossal datasets produced by this method pose a new challenge for life science researchers, who commonly have little statistical or computational training. The processing of sequencing data commonly requires the development of custom workflows that can run in parallel on university computing clusters. Furthermore, the quality control and statistical analysis of these data requires specialist knowledge and purpose-built software packages. This project demonstrates the development of a pipeline for processing RNA-seq datasets on the computing clusters, such as the Imperial College Research Computing Service (RCS), and basic statistical analysis of the normalised data. This will involve: Quality control and trimming of raw RNA-seq reads Alignment of reads to the human reference genome Conversion of aligned reads to a matrix of gene counts Downstream statistical analysis: Data normalisation Unsupervised analysis (e.g. PCA) Differential expression and enrichment using edgeR","title":"Description"},{"location":"#requirements","text":"","title":"Requirements"},{"location":"#academic","text":"Familiarity with bash (A course such as \" The Linux Command Line for Scientific Computing \", hosted by the Imperial Research Computing & Data Science Team, would be provide a suitable background.) Familiarity with R programming language Familiarity with computing clusters","title":"Academic"},{"location":"#system","text":"Program Version R Any Anaconda >=4.13.0 Access to Imperial's HPC Last updated: 29-07-2022","title":"System"},{"location":"#learning-outcomes","text":"Upon completion of this tutorial, students will be able to: parallelise bioinformatics tools on computing clusters, such as the Imperial RCS develop a reproducible pipeline Tools used achieve this as part of the exemplar include: Nextflow , a workflow management system that makes it easy to develop data-driven pipelines. Conda , an package management system that allows you to share your local environment with others. Docker , an application for packaging dependencies into a virtual container. Git/GitHub , a version control system that integrates with nextflow to make pipelines shareable. Continous integration , a practice of automatic code testing.","title":"Learning Outcomes"},{"location":"#getting-started","text":"To get started using the pipeline, install Nextflow and either of Docker , Singularity or Conda . Then, the following command can be used to run the pipeline locally using a small test dataset: nextflow run -r main ImperialCollegeLondon/ReCoDE_rnaseq_pipeline -profile test,docker Note that this pipeline is not designed to handle all types of RNA-seq data (e.g. it was not designed for paired-read data). If you have large amounts of RNA-seq data to process, we recommend using the nextflow-core RNA-seq pipeline . To download the pipelines, install Git and clone this repository from the command line, using the following code: git clone https://github.com/ImperialCollegeLondon/ReCoDE_rnaseq_pipeline.git To run the first two pipelines, you will need to install the command line applications listed in environment.yml . Alternatively, you can use conda to install these applications, using the following code: conda env create -f environment.yml conda activate recode_rnaseq This conda environment also contains Nextflow, which we will be using to create the final iteration of the data processing pipeline. To run the nextflow pipeline, you will also need either Docker or Conda installed. The scripts in this repository are designed for use with the Imperial computing cluster. If you do not have access to the cluster, you might be able to adapt the code to your own cluster's configuration. Alternatively, the primary pipeline uses nextflow, which is adaptable to many different platforms. You could run the nextflow pipeline on your local computer, or configure it to run on another cluster or even the cloud. Some of these tools are not available on windows. If you wish to run the basic pipeline discussed in this document but you use Windows, you could use the Windows Subsystem for Linux for the course or work on a computing cluster, such as Imperial's high performance computing cluster . The downstream analysis steps for RNA-seq data require less compute power and often need a more customised workflow. So, this is demonstrated separately in an R markdown notebook. You can run this notebook, located at notebooks/downstream_analysis.Rmd , or you can view a complete markdown version of the notebook in docs/downstream_analysis.md .","title":"Getting started"},{"location":"#project-structure","text":"In this exemplar, we set up three versions of the same pipeline, that process RNA sequencing data. They are available in docs , along with the background information necessary to follow each pipeline. Each of these pipeline generates .count files in their final step, which counts how many RNA sequences map to each gene for each sample. We created multiple versions of the pipeline to show the different ways in which researchers might want to process their data. They are as follows: simple_pipeline : This is the simplest version of the pipeline. It is a bash script that can be run straight from the command line. It will execute each stage of the pipeline in sequentially. Pros: Simple to follow and execute Cons: slow for a large number of samples if there is an error for a single sample the entire pipeline will fail parallelised_pipeline : This builds on the simple pipeline to run the pipeline on the cluster. It still uses bash scripts to orchestrate the pipeline, however it also allows each sample to be run in parallel on the cluster. This version of the pipeline is therefore a lot faster! Pros: a lot faster than simple_pipeline Cons: unwieldy to use for large numbers of samples nextflow_pipeline : This is the most advanced version of the pipeline. Instead of relying on bash scripts, that could fail without warning, it uses the nextflow workflow manager to run the pipeline stages either on your local computer or on a computing cluster. Pros: automatically orchestrate the running of large numbers of samples run seamlessly both locally, on computing clusters and on cloud platforms easily shared with others Cons: takes more time to set up than the simple or parallelised pipeline We created markdown documents, available in this directory, for each of these pipelines. These documents explain the pipelines and link to external resources in case you want to learn more. We suggest you go through the documentation in the order above, as each one builds upon the last. The downstream analysis uses the output of these pipelines to investigate the data. The dataset is a lot smaller at this point, so this stage of the analysis can be accomplished locally without the use of the cluster. The code for this stage is contained within the notebooks/ directory. The code is written in an Rmarkdown document, which has been run and stored as a markdown document within this directory.","title":"Project Structure"},{"location":"background_nextflow_pipeline/","text":"What is nextflow? nextflow is a workflow management system that allows users to orchestrate the running of scripts into a single cohesive pipeline. nextflow pipelines allow reproducibility and can be easily shared and used by others. nextflow is a DSL (domain specific language) with simple methods for combining scripts into fully-fledged pipelines. For detailed information, see the nextflow site . In this document we will discuss features of nextflow that are useful for coordinating our simple RNA-seq pipeline, but these explanations will be brief and will only scratch the surface of what nextflow can do. We will explain the main concepts alongside the building of our simple RNA-seq pipeline. If you want a more in-depth explanation of writing nextflow code, you can read the nextflow documentation or look for more detailed guides. nextflow can integrate lots of different types of scripts, including those written in bash, Python and R. So, you aren't required to fully learn a new language in order to create nextflow pipelines, you can instead reuse existing scripts and workflows. Our previous pipelines both made use of the same scripts in the bin/ directory, but they used them in slightly different ways. In this document we will use these same scripts, but we will orchestrate them using nextflow. When we wrote the simple local pipeline and the parallelised pipeline, we had to modify the scripts in order to make them compatible with the Imperial cluster. If we were to send this pipeline to a researcher at another university, they might have some difficulty adapting it. nextflow is aware of common cluster schedulers, like PBS and slurm, so it can worry about queueing jobs on the cluster for us! We can write code that runs on our local computer, then export it to a computing cluster to do the heavy lifting without changing anything about the pipeline itself. This is possible because we can give nextflow a config file specifying how it can interact with our specific computing cluster. Many institutions have pre-built config files , so you may not even need to write your own! For our pipeline, we are largely motivated to use nextflow because: i) we don't want to have to worry about setting up our scripts as jobs on the cluster, like we did in docs/parallelised_pipeline.md ; ii) setting up our pipeline with nextflow will make it easier for others to run it, especially if they use a different computing cluster; iii) nextflow will make sure each processing stage produces the expected outputs, and make it far easier to identify the source of the problem. We will also see that nextflow makes it easier to work with environment managers like conda.","title":"Nextflow Pipeline - Background"},{"location":"background_nextflow_pipeline/#what-is-nextflow","text":"nextflow is a workflow management system that allows users to orchestrate the running of scripts into a single cohesive pipeline. nextflow pipelines allow reproducibility and can be easily shared and used by others. nextflow is a DSL (domain specific language) with simple methods for combining scripts into fully-fledged pipelines. For detailed information, see the nextflow site . In this document we will discuss features of nextflow that are useful for coordinating our simple RNA-seq pipeline, but these explanations will be brief and will only scratch the surface of what nextflow can do. We will explain the main concepts alongside the building of our simple RNA-seq pipeline. If you want a more in-depth explanation of writing nextflow code, you can read the nextflow documentation or look for more detailed guides. nextflow can integrate lots of different types of scripts, including those written in bash, Python and R. So, you aren't required to fully learn a new language in order to create nextflow pipelines, you can instead reuse existing scripts and workflows. Our previous pipelines both made use of the same scripts in the bin/ directory, but they used them in slightly different ways. In this document we will use these same scripts, but we will orchestrate them using nextflow. When we wrote the simple local pipeline and the parallelised pipeline, we had to modify the scripts in order to make them compatible with the Imperial cluster. If we were to send this pipeline to a researcher at another university, they might have some difficulty adapting it. nextflow is aware of common cluster schedulers, like PBS and slurm, so it can worry about queueing jobs on the cluster for us! We can write code that runs on our local computer, then export it to a computing cluster to do the heavy lifting without changing anything about the pipeline itself. This is possible because we can give nextflow a config file specifying how it can interact with our specific computing cluster. Many institutions have pre-built config files , so you may not even need to write your own! For our pipeline, we are largely motivated to use nextflow because: i) we don't want to have to worry about setting up our scripts as jobs on the cluster, like we did in docs/parallelised_pipeline.md ; ii) setting up our pipeline with nextflow will make it easier for others to run it, especially if they use a different computing cluster; iii) nextflow will make sure each processing stage produces the expected outputs, and make it far easier to identify the source of the problem. We will also see that nextflow makes it easier to work with environment managers like conda.","title":"What is nextflow?"},{"location":"background_parallelised_pipeline/","text":"Parallelised Pipeline - Background Computing clusters Often, as is often the case with our RNA-seq data, we need more computing power than is available on a typical laptop. RNA-seq data can take an extremely long time to process on a single machine. Furthermore, lots of memory (RAM) is required for running alignment tools. You might also find that you just don't have enough space to store a large dataset on your computer! For this reason, many universities have computing clusters that can help overcome these issues. Computing clusters are a set of computers that are linked together by a network so that they can work together. Each of the computers in the cluster is referred to as a node. These clusters are usually set up with some sort of job scheduler, which allows researchers to submit code to run as a \"job\"; the scheduler will find a slot on one or more of the nodes on which the code can be run. The Imperial computing cluster uses the Portable Batch System (PBS) as a scheduler. In this document, we will demonstrate how the RNA-seq pipeline can be set up to run on the Imperial computing cluster. We will expect that you will have had some previous exposure to running jobs on the cluster, for instance by taking the course Introduction to HPC at Imperial hosted by the Research Computing and Data Science team. If you have access to a cluster other than the Imperial cluster, you could try adapting the scripts for compatibility with your cluster's scheduler. If you do not have access to a computing cluster, it might be difficult to follow along with this document; we suggest trying to understand the strengths and limitations of setting the pipeline up on the cluster, then moving on to docs/nextflow_pipeline.md which describes the final iteration of our pipeline, and can be run without access to a computing cluster. How to run scripts on the cluster The Imperial cluster uses the PBS scheduler. A job can be started by running the qsub command to queue a script to run, for instance as follows: qsub my_script.pbs The script is setup in a .pbs file. This script is very similar to a standard bash script, however it must contain some extra information that tells the scheduler what computing resources the script will need. The key settings you will need to change are the length of time the job will require, the number of cores that should be allocated to the job and the total memory the job will need. For instance, if you were to run a job for 2 hours, with 3 cores and 4GB of memory, you could do so by including the following lines near the top of your .pbs file: #PBS -lselect=1:ncpus=3:mem=4gb #PBS -lwalltime=02:00:00 The other parameter ( lselect ) is set to 1, indicating the job will run on a single node. It is usually best to stick with a single node, unless you are using an application that can make use of multiple. Note that there are many more options we can send to the scheduler, these are just a few that we will commonly want to manipulate. Also, we can pass parameters to PBS in multiple ways. We can include them in our script, as we have done above. This is convenient because we can store our parameters with our script. We know the parameters we used to run the pipeline, and we can use them again next time. Alternatively, these parameters can be passed as arguments to qsub using the command line. This can be useful sometimes, but you might want to save a log of the commands you used somewhere! There are difference \"resources\" available on the computing cluster. For instance, at time of writing, the \"throughput72\" resource on the Imperial computing cluster is applicable for jobs that run on a single node, use 1-8 cores, use 1-100GB memory and run for 9-72 hours. Generally, you should pick the smallest runtime, cores and memory that will allow your code to run, because this will lead to the shortest queue time. Most clusters have a variety of resources available. Some may be optimised for smaller jobs; others may be setup to accommodate high memory requirements or the use of many cores; some will allow access to additional resources, such as GPUs. The above parameters are great for running a single task on the cluster. Sometimes, however, we might have a sequence of jobs that we want to run, one after the other. Say we want to run three programs, one after the other, named A and B. We could create a .pbs script that runs these tasks sequentially, then submit this to the PBS scheduler. However, A and B might have very different resource requirements. Perhaps A requires lots of cores to run in a reasonable amount of time, while task B requires less processing but needs a lot of memory. For this use case, we can set up job dependencies to tell the PBS scheduler to start one job after the other completes. We would first create a .pbs script to run programs A ( a.pbs ) and B ( b.pbs ) with their specific resource requirements, then we could run the following commands: jid_a=\"$(qsub a.pbs)\" qsub -W depend=afterok:\"${jid_a}\" b.pbs The first line includes the code qsub a.pbs , which schedules a.pbs to run like we have seen previously. Running qsub a.pbs returns the ID of the job that has been queued. The remaining code on the first line captures this ID into the variable jid_a . The second line submits b.pbs to the scheduler. We add -W depend=afterok:\"${jid_a}\" to ask the scheduler not to start this second job until the first job, with ID jid_a , has completed successfully. Job dependencies are useful, but could get complex for large numbers of jobs. For instance, in the case of our pipeline, we might want to run the same job for each of our RNA-seq samples. We could have hundreds of samples, meaning we have to manually submit as many jobs! The scheduler's answer to this use case comes in the form of array jobs. Array jobs are created using the J parameter, like so: -J 1-N . The scheduler will submit N jobs; these jobs are otherwise identical, except they each have access to the array ID through the environment variable PBS_ARRAY_INDEX . This variable will be equal to 1 for job 1 and equal to N for job N. Therefore, we can setup a script that uses the array index to process the Nth RNA-seq sample. Array jobs are convenient because they package a large number of tasks into a single job. We will demonstrate its use for processing RNA-seq data in a later section. Environment management conda lets us create isolated environments that include the packages we need and their dependencies. conda lets us more easily share our code with others, because we can share our code alongside a list of software packages that are required. The easiest way to do this is with an environment.yml file, like the one we have created for this exemplar: name: recode_rnaseq channels: - bioconda - conda-forge dependencies: - star=2.7.10a - fastqc=0.11.9 - htseq=0.11.3 - multiqc=1.12 - trim-galore=0.6.7 - sra-tools=2.11.0 - nextflow=22.04.0 - samtools=1.6 If we run the code conda create env -f environment.yml , it creates an environment with the name \"recode_rnaseq\". To create this environment, conda installs all the packages listed as dependencies and makes sure that all of the package versions are compatible with each other. conda knows where these packages are located because we supply the channels bioconda , a repository of bioinformatics software packages, and conda-forge , another repository for open source packages. conda has some limitations though. In our case, we want to create a reproducible pipeline that can be run easily by anyone. For instance, we might find that less common packages are not available on conda. Or, we might find that the packages we want are only available on certain operating systems; STAR, for instance, cannot run on Windows. We can avoid some of these limitations with docker, that is also well-integrated with nextflow. conda allows us to install a set of packages into an environment that is essentially overlaid on top of our current environment. When you activate a conda environment, you still have access to programs that you installed separately to conda. Therefore, when you run code using conda, your environment is not entirely isolated from your host system. docker, on the other hand, allows you to work with containers. Containers are similar to conda environments in the sense that they contain the software and dependencies that you need to run your code. They are different, though, because containers are entirely isolated from your host system. This can make them more consistent, because users will get the same results regardless of their environment or operating system.","title":"Parallelised Pipeline - Background"},{"location":"background_parallelised_pipeline/#parallelised-pipeline-background","text":"","title":"Parallelised Pipeline - Background"},{"location":"background_parallelised_pipeline/#computing-clusters","text":"Often, as is often the case with our RNA-seq data, we need more computing power than is available on a typical laptop. RNA-seq data can take an extremely long time to process on a single machine. Furthermore, lots of memory (RAM) is required for running alignment tools. You might also find that you just don't have enough space to store a large dataset on your computer! For this reason, many universities have computing clusters that can help overcome these issues. Computing clusters are a set of computers that are linked together by a network so that they can work together. Each of the computers in the cluster is referred to as a node. These clusters are usually set up with some sort of job scheduler, which allows researchers to submit code to run as a \"job\"; the scheduler will find a slot on one or more of the nodes on which the code can be run. The Imperial computing cluster uses the Portable Batch System (PBS) as a scheduler. In this document, we will demonstrate how the RNA-seq pipeline can be set up to run on the Imperial computing cluster. We will expect that you will have had some previous exposure to running jobs on the cluster, for instance by taking the course Introduction to HPC at Imperial hosted by the Research Computing and Data Science team. If you have access to a cluster other than the Imperial cluster, you could try adapting the scripts for compatibility with your cluster's scheduler. If you do not have access to a computing cluster, it might be difficult to follow along with this document; we suggest trying to understand the strengths and limitations of setting the pipeline up on the cluster, then moving on to docs/nextflow_pipeline.md which describes the final iteration of our pipeline, and can be run without access to a computing cluster.","title":"Computing clusters"},{"location":"background_parallelised_pipeline/#how-to-run-scripts-on-the-cluster","text":"The Imperial cluster uses the PBS scheduler. A job can be started by running the qsub command to queue a script to run, for instance as follows: qsub my_script.pbs The script is setup in a .pbs file. This script is very similar to a standard bash script, however it must contain some extra information that tells the scheduler what computing resources the script will need. The key settings you will need to change are the length of time the job will require, the number of cores that should be allocated to the job and the total memory the job will need. For instance, if you were to run a job for 2 hours, with 3 cores and 4GB of memory, you could do so by including the following lines near the top of your .pbs file: #PBS -lselect=1:ncpus=3:mem=4gb #PBS -lwalltime=02:00:00 The other parameter ( lselect ) is set to 1, indicating the job will run on a single node. It is usually best to stick with a single node, unless you are using an application that can make use of multiple. Note that there are many more options we can send to the scheduler, these are just a few that we will commonly want to manipulate. Also, we can pass parameters to PBS in multiple ways. We can include them in our script, as we have done above. This is convenient because we can store our parameters with our script. We know the parameters we used to run the pipeline, and we can use them again next time. Alternatively, these parameters can be passed as arguments to qsub using the command line. This can be useful sometimes, but you might want to save a log of the commands you used somewhere! There are difference \"resources\" available on the computing cluster. For instance, at time of writing, the \"throughput72\" resource on the Imperial computing cluster is applicable for jobs that run on a single node, use 1-8 cores, use 1-100GB memory and run for 9-72 hours. Generally, you should pick the smallest runtime, cores and memory that will allow your code to run, because this will lead to the shortest queue time. Most clusters have a variety of resources available. Some may be optimised for smaller jobs; others may be setup to accommodate high memory requirements or the use of many cores; some will allow access to additional resources, such as GPUs. The above parameters are great for running a single task on the cluster. Sometimes, however, we might have a sequence of jobs that we want to run, one after the other. Say we want to run three programs, one after the other, named A and B. We could create a .pbs script that runs these tasks sequentially, then submit this to the PBS scheduler. However, A and B might have very different resource requirements. Perhaps A requires lots of cores to run in a reasonable amount of time, while task B requires less processing but needs a lot of memory. For this use case, we can set up job dependencies to tell the PBS scheduler to start one job after the other completes. We would first create a .pbs script to run programs A ( a.pbs ) and B ( b.pbs ) with their specific resource requirements, then we could run the following commands: jid_a=\"$(qsub a.pbs)\" qsub -W depend=afterok:\"${jid_a}\" b.pbs The first line includes the code qsub a.pbs , which schedules a.pbs to run like we have seen previously. Running qsub a.pbs returns the ID of the job that has been queued. The remaining code on the first line captures this ID into the variable jid_a . The second line submits b.pbs to the scheduler. We add -W depend=afterok:\"${jid_a}\" to ask the scheduler not to start this second job until the first job, with ID jid_a , has completed successfully. Job dependencies are useful, but could get complex for large numbers of jobs. For instance, in the case of our pipeline, we might want to run the same job for each of our RNA-seq samples. We could have hundreds of samples, meaning we have to manually submit as many jobs! The scheduler's answer to this use case comes in the form of array jobs. Array jobs are created using the J parameter, like so: -J 1-N . The scheduler will submit N jobs; these jobs are otherwise identical, except they each have access to the array ID through the environment variable PBS_ARRAY_INDEX . This variable will be equal to 1 for job 1 and equal to N for job N. Therefore, we can setup a script that uses the array index to process the Nth RNA-seq sample. Array jobs are convenient because they package a large number of tasks into a single job. We will demonstrate its use for processing RNA-seq data in a later section.","title":"How to run scripts on the cluster"},{"location":"background_parallelised_pipeline/#environment-management","text":"conda lets us create isolated environments that include the packages we need and their dependencies. conda lets us more easily share our code with others, because we can share our code alongside a list of software packages that are required. The easiest way to do this is with an environment.yml file, like the one we have created for this exemplar: name: recode_rnaseq channels: - bioconda - conda-forge dependencies: - star=2.7.10a - fastqc=0.11.9 - htseq=0.11.3 - multiqc=1.12 - trim-galore=0.6.7 - sra-tools=2.11.0 - nextflow=22.04.0 - samtools=1.6 If we run the code conda create env -f environment.yml , it creates an environment with the name \"recode_rnaseq\". To create this environment, conda installs all the packages listed as dependencies and makes sure that all of the package versions are compatible with each other. conda knows where these packages are located because we supply the channels bioconda , a repository of bioinformatics software packages, and conda-forge , another repository for open source packages. conda has some limitations though. In our case, we want to create a reproducible pipeline that can be run easily by anyone. For instance, we might find that less common packages are not available on conda. Or, we might find that the packages we want are only available on certain operating systems; STAR, for instance, cannot run on Windows. We can avoid some of these limitations with docker, that is also well-integrated with nextflow. conda allows us to install a set of packages into an environment that is essentially overlaid on top of our current environment. When you activate a conda environment, you still have access to programs that you installed separately to conda. Therefore, when you run code using conda, your environment is not entirely isolated from your host system. docker, on the other hand, allows you to work with containers. Containers are similar to conda environments in the sense that they contain the software and dependencies that you need to run your code. They are different, though, because containers are entirely isolated from your host system. This can make them more consistent, because users will get the same results regardless of their environment or operating system.","title":"Environment management"},{"location":"background_simple_local_pipeline/","text":"A brief introduction to RNA-sequencing RNA is a nucleic acid that is similar to DNA. In cells, DNA acts as a long-term storage of genetic information. RNA, on the other hand, has very different functions. One form of RNA, messenger RNA (mRNA), copies the genetic information encoded as DNA and carries it out of the nucleus; here, the cell can use the mRNA sequences to synthesise proteins. When the information from a gene is copied by RNA, we say that the gene has been expressed. This flow of genetic information from DNA to RNA to protein is known as the central dogma of molecular biology. With the advent of the Human Genome Project, we can now determine the sequence of an organism's entire genome. This information has been invaluable to life science researchers; for instance, genome-wide association studies are used to identify genetic mutations that are associated with traits, such as diseases. However, the genome remains relatively static throughout the life of most organisms, so it cannot tell us about the current state of a biological system. The expression of genes, on the other hand, is a lot more dynamic and will change in response to stimuli. For instance, we could identify genes that are activated in a particular disease by comparing the gene expression in healthy people to individuals with the disease. RNA sequencing allows us to elucidate the sequence of a set of RNA molecules in a sample. Given that we know the genome sequence of the organism we are studying, we can find out where each RNA molecule came from in the genome. We are commonly most interested in mRNA molecules, which copy the sequences of genes and use this information to synthesise proteins in the cell. By counting the number of sequences that map back to each gene in an organism, we can quantify gene expression for the genes. Having done these processing stages, we can use various downstream analysis strategies to learn more about the biology of a system. For more information, you can explore the notebook docs/downstream_analysis.md in this repository. The diagram below summarises the process of an RNA-seq experiment. We consider there to be three main stages: Performing RNA sequencing. In this stage we generate a list of sequences in a sample. Processing the data. This process could take many forms, but we will focus on quantifying the number of RNA sequences that originated from each gene. This notebook will focus on this stage, using simple bash scripts and open source tools to perform the data processing steps. Analysing the data. There are lots of ways we could analyse the data, but we will focus on using R to perform some of the most basic and popular analyses. See the docs/downstream_analysis.md notebook for more details. If you are interested in learning more about RNA-seq and other methods for measuring gene expression, you could start with the following review . This video also provides a good introduction to the technique.","title":"Simple Pipeline - Background"},{"location":"background_simple_local_pipeline/#a-brief-introduction-to-rna-sequencing","text":"RNA is a nucleic acid that is similar to DNA. In cells, DNA acts as a long-term storage of genetic information. RNA, on the other hand, has very different functions. One form of RNA, messenger RNA (mRNA), copies the genetic information encoded as DNA and carries it out of the nucleus; here, the cell can use the mRNA sequences to synthesise proteins. When the information from a gene is copied by RNA, we say that the gene has been expressed. This flow of genetic information from DNA to RNA to protein is known as the central dogma of molecular biology. With the advent of the Human Genome Project, we can now determine the sequence of an organism's entire genome. This information has been invaluable to life science researchers; for instance, genome-wide association studies are used to identify genetic mutations that are associated with traits, such as diseases. However, the genome remains relatively static throughout the life of most organisms, so it cannot tell us about the current state of a biological system. The expression of genes, on the other hand, is a lot more dynamic and will change in response to stimuli. For instance, we could identify genes that are activated in a particular disease by comparing the gene expression in healthy people to individuals with the disease. RNA sequencing allows us to elucidate the sequence of a set of RNA molecules in a sample. Given that we know the genome sequence of the organism we are studying, we can find out where each RNA molecule came from in the genome. We are commonly most interested in mRNA molecules, which copy the sequences of genes and use this information to synthesise proteins in the cell. By counting the number of sequences that map back to each gene in an organism, we can quantify gene expression for the genes. Having done these processing stages, we can use various downstream analysis strategies to learn more about the biology of a system. For more information, you can explore the notebook docs/downstream_analysis.md in this repository. The diagram below summarises the process of an RNA-seq experiment. We consider there to be three main stages: Performing RNA sequencing. In this stage we generate a list of sequences in a sample. Processing the data. This process could take many forms, but we will focus on quantifying the number of RNA sequences that originated from each gene. This notebook will focus on this stage, using simple bash scripts and open source tools to perform the data processing steps. Analysing the data. There are lots of ways we could analyse the data, but we will focus on using R to perform some of the most basic and popular analyses. See the docs/downstream_analysis.md notebook for more details. If you are interested in learning more about RNA-seq and other methods for measuring gene expression, you could start with the following review . This video also provides a good introduction to the technique.","title":"A brief introduction to RNA-sequencing"},{"location":"downstream_analysis/","text":"Downstream analyses Jack Gisby 2022-06-14 Loading the data Unsupervised analysis and visualisation Principal components analysis Differential expression This markdown document shows how we can perform basic downstream analysis on raw RNA-seq counts. These counts were produced by the data processing pipeline developed in this repository applied to RNA-seq reads measured from soybean plants grown at ambient or elevated ozone levels. If you want to learn more about the preprocessing pipeline, see the documents describing its development in the docs/ directory. The raw counts files are a lot smaller than the original reads (that were stored in fasta files). They are small enough that we can read the raw counts into R on a standard home computer. After normalising the raw counts data, we will perform some basic analyses to investigate the transcriptome of the soybean samples. Loading the data We load the names of the samples from data/files.txt into sample_names before extracting the raw counts for each sample. There is a counts file for each sample, but it would be more convenient to group these together into a single data matrix. Each of these files contains two columns; the first represents gene IDs and the second indicates how many of the reads for that sample mapped to that gene. We first create a dataframe, counts , that contains a single column: the gene IDs that we extract from the counts file for the first sample (SRR391535). Then, we loop through each of the samples, reading in that sample\u2019s count file and merging them into the matrix counts . Finally, we can see that we have created a single matrix describing how many reads map to each gene for each sample. # get the sample names sample_names <- read.csv ( \"../data/files.txt\" , header = FALSE )[[ 1 ]] # a dataframe for all of the counts counts <- data.frame ( gene_id = fread ( \"../3_nextflow_pipeline_results/g_count/SRR391535.counts\" )[[ 1 ]]) # for each sample for ( s in sample_names ) { # get the counts from htseq sample_counts <- fread ( paste0 ( \"../3_nextflow_pipeline_results/g_count/\" , s , \".counts\" )) colnames ( sample_counts ) <- c ( \"gene_id\" , s ) # remove counts for reads that did not map to a gene sample_counts <- sample_counts [ which ( ! grepl ( \"__\" , sample_counts [[ 1 ]])) ,] # merge with the main dataframe counts <- merge ( counts , sample_counts , by = \"gene_id\" ) } # move the gene_id column to the rownames rownames ( counts ) <- counts $ gene_id counts <- subset ( counts , select = - gene_id ) # view the combined matrix print ( head ( counts )) ## SRR391535 SRR391536 SRR391537 SRR391538 SRR391539 SRR391541 ## 1-3-1A 87 41 59 88 79 98 ## 1-3-1B 242 150 212 289 249 288 ## 100801599 22 9 859 1455 1421 24 ## 100805863 0 0 0 0 0 0 ## 13-1-1 110 39 163 121 284 194 ## 14-1-1 21 26 65 14 63 33 In the following chunk, we create a dataframe, sample_names , describing the condition for each of the samples. Three of the samples were grown at ambient ozone levels, while the other three were grown with elevated ozone. sample_info <- data.frame ( condition = c ( \"ambient\" , \"ambient\" , \"elevated\" , \"elevated\" , \"elevated\" , \"ambient\" )) rownames ( sample_info ) <- sample_names print ( sample_info ) ## condition ## SRR391535 ambient ## SRR391536 ambient ## SRR391537 elevated ## SRR391538 elevated ## SRR391539 elevated ## SRR391541 ambient The Bioconductor project has a package implementing a special container for this sort of data. We can store the counts data in the SummarizedExperiment object, and we can also store information pertaining to the rows (genes) and columns (samples). For more information on this data container, see the SummarizedExperiment vignette . se <- SummarizedExperiment ( counts , colData = sample_info ) assayNames ( se ) <- \"counts\" print ( se ) ## class: SummarizedExperiment ## dim: 54361 6 ## metadata(0): ## assays(1): counts ## rownames(54361): 1-3-1A 1-3-1B ... ZTL1 ZTL2 ## rowData names(0): ## colnames(6): SRR391535 SRR391536 ... SRR391539 SRR391541 ## colData names(1): condition From this container, we can access information on our samples. colData ( se ) ## DataFrame with 6 rows and 1 column ## condition ## <character> ## SRR391535 ambient ## SRR391536 ambient ## SRR391537 elevated ## SRR391538 elevated ## SRR391539 elevated ## SRR391541 ambient And we can extract the counts data as a matrix. # view the first ten rows of the counts matrix assay ( se )[ 1 : 10 ,] ## SRR391535 SRR391536 SRR391537 SRR391538 SRR391539 SRR391541 ## 1-3-1A 87 41 59 88 79 98 ## 1-3-1B 242 150 212 289 249 288 ## 100801599 22 9 859 1455 1421 24 ## 100805863 0 0 0 0 0 0 ## 13-1-1 110 39 163 121 284 194 ## 14-1-1 21 26 65 14 63 33 ## 19-1-5 0 0 1 0 0 0 ## 4CL 1309 742 574 860 1279 1445 ## 4CL1 9627 5597 16109 28881 32941 10526 ## 4CL2 1145 556 2686 4424 5729 1407 Unsupervised analysis and visualisation Next, we will attempt to visualise the soybean transcriptomes. Currently, we are storing the raw counts with the SummarizedExperiment object. However, these are not necessarily appropriate for all downstream applications. For instance: Some genes may not be expressed by many/any of the samples. We may not want to include these in our downstream analyses. Analyses that compare different samples or genes may assume that particular normalisation procedures have been applied. Some analyses are designed to work with counts after they have had a transformation applied, such as a log transformation. For each of our analyses, we will consider whether our data has been appropriately processed. The package edgeR is popular for performing differential expression analysis of RNA-seq data. It also has functions for normalising and transforming raw counts data. We will be using it in this notebook. First, we will filter out genes that have low counts in our samples. The function filterByExpr will keep genes that have sufficient counts in the majority of our samples. We will also pass our experimental condition to the group argument, which makes sure there is sufficient counts in both the ambient and elevated sample groups. # select genes to keep fbe_keep <- filterByExpr ( se , group = se $ condition , min.count = 500 ) # remove genes that are not in the list filtered_se <- se [ fbe_keep , ] print ( paste0 ( \"Number of genes before filtering: \" , nrow ( rowData ( se )))) ## [1] \"Number of genes before filtering: 54361\" print ( paste0 ( \"Number of genes after filtering: \" , nrow ( rowData ( filtered_se )))) ## [1] \"Number of genes after filtering: 10727\" Secondly, we will calculate normalisation factors. The function calcNormFactors will do this, however the normalisation will not actually be applied to the counts yet. calcNormFactors transforms the SummarizedExperiment object into an edgeR DGEList object, in which it will store the raw counts and the calculated normalisation factors. edgeR applies TMM normalisation, which accounts for the following factors: - sequencing depth - This refers to the number of reads that have been generated in total. More reads may have been generated for some samples compared to others. It is essential to normalise for sequencing depth before comparing gene expression from different samples. - RNA composition - Highly expressed outliers or contamination can skew some normalisation methods, so RNA composition must be accounted for. - gene length - Longer genes are likely to have more reads map to them. While not relevant to this notebook, if we were to compare expression of different genes within the sample samples we would need to account for gene length. For more information on TMM normalisation, see the following publication . # calculate normalisation factors, including TMM normalisation dge <- calcNormFactors ( filtered_se ) # add the experimental condition as the DGEList's group dge $ samples $ group <- dge $ samples $ condition The SummarizedExperiment can store multiple versions of the same count matrix, for instance with different normalisations or transformations applied. We have stored the raw counts in the first slot, but we can store the normalised data in another slot. For generating visualisations and performing principal components analysis, we will convert the raw counts to counts per million (CPM). So, the raw counts represent number of mapped reads for each gene, while CPM represents the number of reads mapped to each gene per million total mapped reads. The cpm function will also apply the normalisation factors calculated in the previous step log transform the data. # add normalised logCPM data to the summarized experiment object assay ( filtered_se , 2 ) <- cpm ( dge , log = TRUE ) Principal components analysis Principal components analysis (PCA) is a dimensionality reduction that finds vectors (principal components) that represent the greatest proportion of variance in the data. In practice, we can summarise many (1000s) genes in just a few principal components that still contain most of the information in the full transcriptome. We can perform PCA in R using the prcomp function, which uses singular value decomposition to carry out the calculation. The function also centers and scales (standardises) the logCPM data. PCA is sensitive to the variance of the input variables, and different genes have very different ranges of expression; so, standardisation is recommended to keep different genes on comparable scales. # prcomp calculates PCA using singular value decomposition cpm_pca <- prcomp ( t ( assay ( filtered_se , 2 )), center = TRUE , scale. = TRUE ) print ( cpm_pca $ x ) ## PC1 PC2 PC3 PC4 PC5 PC6 ## SRR391535 -84.67757 59.314154 19.071329 -12.818820 -14.725144 2.078164e-13 ## SRR391536 -76.42968 -2.607008 -25.522355 17.769187 35.552955 2.476643e-13 ## SRR391537 82.68334 19.575153 -46.849790 4.746674 -18.867265 1.912030e-13 ## SRR391538 80.39912 3.084402 39.136481 36.215136 5.418837 4.876828e-14 ## SRR391539 59.26173 -15.795781 10.446484 -48.490415 17.783266 1.209415e-13 ## SRR391541 -61.23694 -63.570920 3.717851 2.578239 -25.162647 2.254863e-13 While we could plot the principal components manually, it is convenient to use the ggbiplot package to do it for us. This function additionally calculates the variance explained by each component as a percentage, and plots the orientation of some of the genes in PC-space. It also colours our samples by the experimental condition and draws an ellipse around each group. We can see that we can draw mostly distinct ellipses around each of the experimental conditions, indicating clear differences between the groups. PCA is known as \u201cunsupervised\u201d, because it identifies patterns in the data without knowledge of labels of interest. pca_df <- data.frame ( condition = filtered_se $ condition , cpm_pca $ x ) # use the ggplot package to plot the PCA ggplot ( pca_df , aes ( PC1 , PC2 , col = condition )) + geom_point () Differential expression We have performed an unsupervised analysis, PCA, to visualise our data. This demonstrated clear differences between or groups of interest, motivating us to investigate how the transcriptome differs by the experimental condition. We will now perform differential expression for each of the genes in the transcriptome. This is a supervised method because we will look for differences using the group labels. In a previous step, we generated a DGEList containing the raw counts and the computed normalisation factors. With edgeR , we never need to directly apply these normalisation factors to the data. The models employed by edgeR take the raw counts and normalisation factors directly. These models can work with raw counts directly because they assume that the counts follow a negative binomial distribution. In order to fit these models, edgeR must calculate the technical- and biological-specific variation using the estimateDisp function. The recommended method for a simple two-group comparison is edgeR \u2019s quantile- adjusted conditional maximum likelihood method. This is implemented in the exactTest function. The edgeR Users Guide notes that this test has strong parallels with the Fisher\u2019s exact test. # estimate dispersion parameter dge <- estimateDisp ( dge ) ## Using classic mode. # fit the models et <- exactTest ( dge ) # get the results as a table res <- topTags ( et , n = 60000 ) # print the most significant results in the table print ( head ( res )) ## Comparison of groups: elevated-ambient ## logFC logCPM PValue FDR ## LOC100790507 7.292131 5.709596 1.446266e-222 1.551409e-218 ## 100801599 6.053555 4.926274 1.819215e-156 9.757361e-153 ## LOC100794841 4.251261 5.920580 3.867659e-151 1.382946e-147 ## LOC100798930 4.134225 5.388805 5.977479e-130 1.603010e-126 ## LOC100500316 6.323671 3.626933 3.824606e-124 8.205310e-121 ## LOC100500550 4.716406 4.923224 4.859905e-116 8.688700e-113 To visualise the results, we will create a volcano plot, which makes it easier to identify genes with particularly high fold changes and small P-values. # use the EnhancedVolcano package to visualise the results EnhancedVolcano ( res $ table , lab = rownames ( res $ table ), x = \"logFC\" , y = \"PValue\" , pCutoffCol = \"FDR\" , pCutoff = 0.05 )","title":"Downstream analyses"},{"location":"downstream_analysis/#downstream-analyses","text":"Jack Gisby 2022-06-14 Loading the data Unsupervised analysis and visualisation Principal components analysis Differential expression This markdown document shows how we can perform basic downstream analysis on raw RNA-seq counts. These counts were produced by the data processing pipeline developed in this repository applied to RNA-seq reads measured from soybean plants grown at ambient or elevated ozone levels. If you want to learn more about the preprocessing pipeline, see the documents describing its development in the docs/ directory. The raw counts files are a lot smaller than the original reads (that were stored in fasta files). They are small enough that we can read the raw counts into R on a standard home computer. After normalising the raw counts data, we will perform some basic analyses to investigate the transcriptome of the soybean samples.","title":"Downstream analyses"},{"location":"downstream_analysis/#loading-the-data","text":"We load the names of the samples from data/files.txt into sample_names before extracting the raw counts for each sample. There is a counts file for each sample, but it would be more convenient to group these together into a single data matrix. Each of these files contains two columns; the first represents gene IDs and the second indicates how many of the reads for that sample mapped to that gene. We first create a dataframe, counts , that contains a single column: the gene IDs that we extract from the counts file for the first sample (SRR391535). Then, we loop through each of the samples, reading in that sample\u2019s count file and merging them into the matrix counts . Finally, we can see that we have created a single matrix describing how many reads map to each gene for each sample. # get the sample names sample_names <- read.csv ( \"../data/files.txt\" , header = FALSE )[[ 1 ]] # a dataframe for all of the counts counts <- data.frame ( gene_id = fread ( \"../3_nextflow_pipeline_results/g_count/SRR391535.counts\" )[[ 1 ]]) # for each sample for ( s in sample_names ) { # get the counts from htseq sample_counts <- fread ( paste0 ( \"../3_nextflow_pipeline_results/g_count/\" , s , \".counts\" )) colnames ( sample_counts ) <- c ( \"gene_id\" , s ) # remove counts for reads that did not map to a gene sample_counts <- sample_counts [ which ( ! grepl ( \"__\" , sample_counts [[ 1 ]])) ,] # merge with the main dataframe counts <- merge ( counts , sample_counts , by = \"gene_id\" ) } # move the gene_id column to the rownames rownames ( counts ) <- counts $ gene_id counts <- subset ( counts , select = - gene_id ) # view the combined matrix print ( head ( counts )) ## SRR391535 SRR391536 SRR391537 SRR391538 SRR391539 SRR391541 ## 1-3-1A 87 41 59 88 79 98 ## 1-3-1B 242 150 212 289 249 288 ## 100801599 22 9 859 1455 1421 24 ## 100805863 0 0 0 0 0 0 ## 13-1-1 110 39 163 121 284 194 ## 14-1-1 21 26 65 14 63 33 In the following chunk, we create a dataframe, sample_names , describing the condition for each of the samples. Three of the samples were grown at ambient ozone levels, while the other three were grown with elevated ozone. sample_info <- data.frame ( condition = c ( \"ambient\" , \"ambient\" , \"elevated\" , \"elevated\" , \"elevated\" , \"ambient\" )) rownames ( sample_info ) <- sample_names print ( sample_info ) ## condition ## SRR391535 ambient ## SRR391536 ambient ## SRR391537 elevated ## SRR391538 elevated ## SRR391539 elevated ## SRR391541 ambient The Bioconductor project has a package implementing a special container for this sort of data. We can store the counts data in the SummarizedExperiment object, and we can also store information pertaining to the rows (genes) and columns (samples). For more information on this data container, see the SummarizedExperiment vignette . se <- SummarizedExperiment ( counts , colData = sample_info ) assayNames ( se ) <- \"counts\" print ( se ) ## class: SummarizedExperiment ## dim: 54361 6 ## metadata(0): ## assays(1): counts ## rownames(54361): 1-3-1A 1-3-1B ... ZTL1 ZTL2 ## rowData names(0): ## colnames(6): SRR391535 SRR391536 ... SRR391539 SRR391541 ## colData names(1): condition From this container, we can access information on our samples. colData ( se ) ## DataFrame with 6 rows and 1 column ## condition ## <character> ## SRR391535 ambient ## SRR391536 ambient ## SRR391537 elevated ## SRR391538 elevated ## SRR391539 elevated ## SRR391541 ambient And we can extract the counts data as a matrix. # view the first ten rows of the counts matrix assay ( se )[ 1 : 10 ,] ## SRR391535 SRR391536 SRR391537 SRR391538 SRR391539 SRR391541 ## 1-3-1A 87 41 59 88 79 98 ## 1-3-1B 242 150 212 289 249 288 ## 100801599 22 9 859 1455 1421 24 ## 100805863 0 0 0 0 0 0 ## 13-1-1 110 39 163 121 284 194 ## 14-1-1 21 26 65 14 63 33 ## 19-1-5 0 0 1 0 0 0 ## 4CL 1309 742 574 860 1279 1445 ## 4CL1 9627 5597 16109 28881 32941 10526 ## 4CL2 1145 556 2686 4424 5729 1407","title":"Loading the data"},{"location":"downstream_analysis/#unsupervised-analysis-and-visualisation","text":"Next, we will attempt to visualise the soybean transcriptomes. Currently, we are storing the raw counts with the SummarizedExperiment object. However, these are not necessarily appropriate for all downstream applications. For instance: Some genes may not be expressed by many/any of the samples. We may not want to include these in our downstream analyses. Analyses that compare different samples or genes may assume that particular normalisation procedures have been applied. Some analyses are designed to work with counts after they have had a transformation applied, such as a log transformation. For each of our analyses, we will consider whether our data has been appropriately processed. The package edgeR is popular for performing differential expression analysis of RNA-seq data. It also has functions for normalising and transforming raw counts data. We will be using it in this notebook. First, we will filter out genes that have low counts in our samples. The function filterByExpr will keep genes that have sufficient counts in the majority of our samples. We will also pass our experimental condition to the group argument, which makes sure there is sufficient counts in both the ambient and elevated sample groups. # select genes to keep fbe_keep <- filterByExpr ( se , group = se $ condition , min.count = 500 ) # remove genes that are not in the list filtered_se <- se [ fbe_keep , ] print ( paste0 ( \"Number of genes before filtering: \" , nrow ( rowData ( se )))) ## [1] \"Number of genes before filtering: 54361\" print ( paste0 ( \"Number of genes after filtering: \" , nrow ( rowData ( filtered_se )))) ## [1] \"Number of genes after filtering: 10727\" Secondly, we will calculate normalisation factors. The function calcNormFactors will do this, however the normalisation will not actually be applied to the counts yet. calcNormFactors transforms the SummarizedExperiment object into an edgeR DGEList object, in which it will store the raw counts and the calculated normalisation factors. edgeR applies TMM normalisation, which accounts for the following factors: - sequencing depth - This refers to the number of reads that have been generated in total. More reads may have been generated for some samples compared to others. It is essential to normalise for sequencing depth before comparing gene expression from different samples. - RNA composition - Highly expressed outliers or contamination can skew some normalisation methods, so RNA composition must be accounted for. - gene length - Longer genes are likely to have more reads map to them. While not relevant to this notebook, if we were to compare expression of different genes within the sample samples we would need to account for gene length. For more information on TMM normalisation, see the following publication . # calculate normalisation factors, including TMM normalisation dge <- calcNormFactors ( filtered_se ) # add the experimental condition as the DGEList's group dge $ samples $ group <- dge $ samples $ condition The SummarizedExperiment can store multiple versions of the same count matrix, for instance with different normalisations or transformations applied. We have stored the raw counts in the first slot, but we can store the normalised data in another slot. For generating visualisations and performing principal components analysis, we will convert the raw counts to counts per million (CPM). So, the raw counts represent number of mapped reads for each gene, while CPM represents the number of reads mapped to each gene per million total mapped reads. The cpm function will also apply the normalisation factors calculated in the previous step log transform the data. # add normalised logCPM data to the summarized experiment object assay ( filtered_se , 2 ) <- cpm ( dge , log = TRUE )","title":"Unsupervised analysis and visualisation"},{"location":"downstream_analysis/#principal-components-analysis","text":"Principal components analysis (PCA) is a dimensionality reduction that finds vectors (principal components) that represent the greatest proportion of variance in the data. In practice, we can summarise many (1000s) genes in just a few principal components that still contain most of the information in the full transcriptome. We can perform PCA in R using the prcomp function, which uses singular value decomposition to carry out the calculation. The function also centers and scales (standardises) the logCPM data. PCA is sensitive to the variance of the input variables, and different genes have very different ranges of expression; so, standardisation is recommended to keep different genes on comparable scales. # prcomp calculates PCA using singular value decomposition cpm_pca <- prcomp ( t ( assay ( filtered_se , 2 )), center = TRUE , scale. = TRUE ) print ( cpm_pca $ x ) ## PC1 PC2 PC3 PC4 PC5 PC6 ## SRR391535 -84.67757 59.314154 19.071329 -12.818820 -14.725144 2.078164e-13 ## SRR391536 -76.42968 -2.607008 -25.522355 17.769187 35.552955 2.476643e-13 ## SRR391537 82.68334 19.575153 -46.849790 4.746674 -18.867265 1.912030e-13 ## SRR391538 80.39912 3.084402 39.136481 36.215136 5.418837 4.876828e-14 ## SRR391539 59.26173 -15.795781 10.446484 -48.490415 17.783266 1.209415e-13 ## SRR391541 -61.23694 -63.570920 3.717851 2.578239 -25.162647 2.254863e-13 While we could plot the principal components manually, it is convenient to use the ggbiplot package to do it for us. This function additionally calculates the variance explained by each component as a percentage, and plots the orientation of some of the genes in PC-space. It also colours our samples by the experimental condition and draws an ellipse around each group. We can see that we can draw mostly distinct ellipses around each of the experimental conditions, indicating clear differences between the groups. PCA is known as \u201cunsupervised\u201d, because it identifies patterns in the data without knowledge of labels of interest. pca_df <- data.frame ( condition = filtered_se $ condition , cpm_pca $ x ) # use the ggplot package to plot the PCA ggplot ( pca_df , aes ( PC1 , PC2 , col = condition )) + geom_point ()","title":"Principal components analysis"},{"location":"downstream_analysis/#differential-expression","text":"We have performed an unsupervised analysis, PCA, to visualise our data. This demonstrated clear differences between or groups of interest, motivating us to investigate how the transcriptome differs by the experimental condition. We will now perform differential expression for each of the genes in the transcriptome. This is a supervised method because we will look for differences using the group labels. In a previous step, we generated a DGEList containing the raw counts and the computed normalisation factors. With edgeR , we never need to directly apply these normalisation factors to the data. The models employed by edgeR take the raw counts and normalisation factors directly. These models can work with raw counts directly because they assume that the counts follow a negative binomial distribution. In order to fit these models, edgeR must calculate the technical- and biological-specific variation using the estimateDisp function. The recommended method for a simple two-group comparison is edgeR \u2019s quantile- adjusted conditional maximum likelihood method. This is implemented in the exactTest function. The edgeR Users Guide notes that this test has strong parallels with the Fisher\u2019s exact test. # estimate dispersion parameter dge <- estimateDisp ( dge ) ## Using classic mode. # fit the models et <- exactTest ( dge ) # get the results as a table res <- topTags ( et , n = 60000 ) # print the most significant results in the table print ( head ( res )) ## Comparison of groups: elevated-ambient ## logFC logCPM PValue FDR ## LOC100790507 7.292131 5.709596 1.446266e-222 1.551409e-218 ## 100801599 6.053555 4.926274 1.819215e-156 9.757361e-153 ## LOC100794841 4.251261 5.920580 3.867659e-151 1.382946e-147 ## LOC100798930 4.134225 5.388805 5.977479e-130 1.603010e-126 ## LOC100500316 6.323671 3.626933 3.824606e-124 8.205310e-121 ## LOC100500550 4.716406 4.923224 4.859905e-116 8.688700e-113 To visualise the results, we will create a volcano plot, which makes it easier to identify genes with particularly high fold changes and small P-values. # use the EnhancedVolcano package to visualise the results EnhancedVolcano ( res $ table , lab = rownames ( res $ table ), x = \"logFC\" , y = \"PValue\" , pCutoffCol = \"FDR\" , pCutoff = 0.05 )","title":"Differential expression"},{"location":"running_nextflow_pipeline/","text":"Running the nextflow pipeline Set Up Previously, we created a single conda environment in which we installed all of the packages we needed to run the pipeline. For the nextflow pipeline, we will instead create an environment for each step of the pipeline. By this, we mean that we will separate each step (QC, trimming, indexing, alignment, counting, multiqc) and use an environment that contains only the package we need. For instance, in the QC step we will create an environment that contains only FastQC and its dependencies. Using different environments for each step is useful as the project gets bigger, so that we don't have to create huge and complex conda environments. As we will see when we create the nextflow pipeline, we don't need to set up these environments manually; nextflow will create the environments for us! conda integration is a relatively new feature to nextflow, and it is generally recommended to user docker containers instead. So, we need to make a container for each of the pipeline steps that contains the applications we need. nextflow is helpful in that it will download and activate the containers we specify automatically, but these containers must exist to begin with! Luckily for us, containers have already been created for many common bioinformatics tools. They are created and hosted by the BioContainers project . Try visiting their website and finding a container image for the FastQC program. When we write our pipeline we will show you how to specify the container image that contains the relevant packages. First, though, we will discuss how to create your own containers. We have already found a FastQC container image that we could use, but for the sake of learning we will try and create our own FastQC container. Containers are stored as \"images\", which are instructions on how to assemble a docker container. We can create our own image that contains FastQC, and use this to build our environment as part of our pipeline. After we have downloaded docker and created a dockerfile (you can find ours in the top-level directory of the exemplar), we can begin specifying our image. To do this, we will be creating an image that is similar to the BioContainers FastQC image. The first step in creating the image is to choose a base image. The base image is the docker image you will build your own image on top of. For instance, you may choose to use a minimal linux image to build upon. We will use the base biocontainers image, which uses Ubuntu as its operating system and includes conda within the container. We do this like so: FROM biocontainers/biocontainers:v1.1.0_cv2 Below, we use USER root to switch to the root user, because we need admin permissions to download and setup FastQC. We then download FastQC and its dependency java, unpack it and allow it to be executed. USER root RUN apt-get update && apt-get install -y openjdk-8-jre-headless RUN mkdir -p /opt/fastqc RUN wget https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.9.zip --no-check-certificate -O /opt/fastqc/fastqc_v0.11.9.zip RUN unzip /opt/fastqc/fastqc_v0.11.9.zip -d /opt/fastqc RUN rm /opt/fastqc/fastqc_v0.11.9.zip RUN mv /opt/fastqc/FastQC/* /opt/fastqc RUN rmdir /opt/fastqc/FastQC && chmod +x /opt/fastqc/fastqc Next, we create a link to the program in /usr/local/bin/ and add the directory to our path. Now, when we run the fastqc command within our container, it will run the program. RUN ln -s /opt/fastqc/fastqc /usr/local/bin/fastqc ENV PATH /usr/local/bin:$PATH We would prefer that the container be run with a non-root user. This is considered best practice for security: why run FastQC as an admin when we can do it as a standard user? So, we create and switch to a non-root user for when we actually run the container. RUN adduser --system --no-create-home nonroot USER nonroot To use our docker container, we first have to build it as an image. To do this, we will use the docker build command. For this exemplar, I created an account on dockerhub, which allows users to host and share docker images, and stored the image on dockerhub. So, after building the image, I additionally pushed the image to dockerhub, as so: # docker build -t jackgisby/fastqc . # docker push jackgisby/fastqc This way, you can use my docker image or edit the pipeline to use the image you have created. Usually our next step would be to demonstrate how to access and use the docker image directly, but nextflow will take care of this for us, as we will demonstrate in the following sections. If you want to use docker in your other work, it may be useful to read the docker documentation or a specific course on docker. Running the pipeline We can now start building our nextflow pipeline. We will create a module for each of our processing steps in bin/ and connect them together into a cohesive nextflow pipeline. We will allow users to use either docker or conda to run the pipeline. We will test the pipeline locally before running it on the cluster for the full dataset. Finally, we will look at how we can set up GitHub actions to test the pipeline every time we make a change. We will go over the main features of our workflow, but we will not spend much time providing background to nextflow. It may be useful to consult other resources, like the nextflow documentation , to find out more details or clarify how elements of the pipeline work. Building the pipeline processes The first step in creating our pipeline will be to create a nextflow process to run each of the steps in bin/ . We will create these in the modules/ directory. For instance, a minimal process for running FastQC on a .fastq file might look like this: process FASTQC { input: path fastq output: path \"*_fastqc.html\", emit: fastqc_html path \"*_fastqc.zip\", emit: fastqc_zip script: \"\"\" fastqc -o ./ ./$fastq \"\"\" } The key parts of this process definition include: input - The inputs are the files that are going to be used as input to this process. In this case we want to process a .fastq file, so we specify that we expect a path as input, and we name this \"fastq\". output - The outputs are the files that we expect our program is going to generate. We don't need to specify all of the program's outputs, just the ones that we want to keep. In this case, we want to capture the files that end in \"_fastqc.html\" and those that end in \"_fastqc.zip\"; we achieve this using the wildcard character, \"*\". These files represent the FastQC report that we will later pass to MultiQC in order to view the QC metrics for our .fastq files. We use emit: to name the outputs to make accessing them later easier. The \"*\" wildcard symbol is used, which will match any text. script - The script contains one or more commands to be run as part of the process. This is like a bash script, and it can access the variables we defined above in the process. For instance, we use the fastq variable to access the location of the .fastq file that needs to be processed. This script will run fastqc on this .fastq file and save the results to the working directory. We will discuss later how to pass the inputs and recover the outputs from these processes. You might notice that the actual module, modules/fastqc.nf is slightly more complex, demonstrated below: process FASTQC { label \"long\" publishDir \"$params.outdir/a_fastqc/\", mode: params.publish_dir_mode conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null) // use a custom container rather than biocontainers // container \"quay.io/biocontainers/fastqc:0.11.9--0\" container \"jackgisby/fastqc:v1\" input: tuple val(accession), path(fastq) output: path \"*_fastqc.html\", emit: fastqc_html path \"*_fastqc.zip\", emit: fastqc_zip script: \"\"\" mkdir fastqc_tmp $baseDir/bin/fastqc.sh \\ \"./\" \\ \"$fastq\" \\ \"./fastqc_tmp\" \"\"\" } One major addition is that our process now accesses variables stored within params . This variable contains parameters that let us modify how our pipeline runs. We will discuss how we can set and modify these parameters later. The key additions to the module are as follows: label - We can add our processes to different groups using labels. We will make use of labels later when we use them to decide which cluster resources we will allocate to each process. publishDir - Even though we have specified expected outputs, nextflow does not necessarily make it easy for us to retrieve our results. nextflow stores all of the input and output files into a directory named work/ . This is where nextflow does its processing, and it makes it easier for nextflow to isolate each of the processes when they may be running concurrently. If we specify publishDir , nextflow will copy the expected output files to a directory of our choosing. We specify the directory as $params.outdir/a_fastqc/ - i.e. we want the results saved to the a_fastqc directory within the folder specified by $params.outdir . We set the mode variable which controls the method nextflow uses to make the results available. conda - If we are using conda to get the FastQC program, then this tells nextflow where it can find FastQC. The variable params.enable_conda variable indicates whether we are using conda. If so, we tell nextflow to use a specific version of FastQC available from Bioconda (\"bioconda::fastqc=0.11.9\"), else we use null to indicate nextflow should not download a conda environment. container - If we are not using conda, we are going to use containers instead. This command specifies which container nextflow should set up to run the analysis. To do this, we could use the container \"quay.io/biocontainers/fastqc:0.11.9--0\", which is the URL of the biocontainers FastQC image. But, for this process, we will instead use the docker image we created earlier: \"jackgisby/fastqc:v1\". This will get version 1 of the FastQC container I published to dockerhub. input - We changed the input so that it accepts both the path of the .fastq file, but also its sample ID. This might be useful for naming the outputs using the variable accession . script - Instead of running FastQC directly, we are now using the script bin/fastqc.sh instead. In general I don't recommend doing this, but we will do it this way for consistency with the previous iterations of the pipeline that we developed! Remember that nextflow can also run other types of scripts - in this section we could alternatively run a Python or an R script. Note that we use the baseDir variable: this is the base directory of the nextflow pipeline (i.e. the top-level directory of this repository). We have created similar processes in the modules/ directory for each of our pipeline steps. Many of these are very similar, we just run the relevant script in bin/ while specifying the input and output files that are specific to the program that has been run. Next we will move on to specifying the sequence of the analysis, so it might be worthwhile to look at the other processes and try and understand what is going on. You should be able to match up the inputs and outputs to the files we generated for the previous pipelines. Putting the pipeline together We will now create a script, workflows/nextflow_pipeline.nf , to create a cohesive workflow. This will simply join the processes we have created into a single script. With nextflow, we don't explicitly state the order tasks should be run. We simply define how the data flows between each of our processes, and nextflow will decide on the running order and will run steps in parallel where necessary! The first part of our workflow script simply imports the processes that we generaed previously, for example: include { FASTQC } from \"$baseDir/modules/fastqc.nf\" Instead of defining a process, we now define a workflow. Workflows are used for orchestrating the flow of data between multiple processes. A more complex project might even define workflows within other workflows, but our use case is a lot simpler. We define a workflow called PROCESS_RNASEQ that will represent our end-to-end pipeline. workflow PROCESS_RNASEQ { } Within this workflow, we start by getting the input data. nextflow uses variables, called \"channels\", to describe how data will flow through a workflow. There are two different types of channels. \"Value\" channels are like regular variables we would define in bash, Python or R. It contains a single value and can be passed to nextflow processes to change how they run. The other type of channel is a \"Queue\" channel. These channels typically contain multiple inputs that each need to be processed. There are multiple ways of defining these channels, but we will use fromPath . This allows us to create a channel from a set of input files, like our .fastq files. To create our queue, we define a channel called input_fastq using Channel.fromPath with the .fastq files as input. We specify that the channel should look into the params.fastqc_dir directory and include any files within it that end with \".fastq.gz\" using the \"*\" wildcard character. This on its own is enough to setup a channel containing all of the .fastq files. We do an additional step, using the .map command, that adds the name of the sample to the channel. In the process modules, this lets us get both the sample name and path. input_fastq = Channel.fromPath(\"${params.fastqc_dir}/*.fastq.gz\").map { tuple( it.name.split('\\\\.')[0], it ) } In the next step, we use the FASTQC as if it was a function by running: FASTQC(input_fastq) . nextflow understands that, since there are multiple files in the input_fastq channel, it must run the FASTQC process for each element of the channel. This is the behaviour we want, because we need to run fastqc on each sample individually. Note that, just because we call the FASTQC process first, doesn't mean this will be the first thing nextflow runs. For instance, the first process launched might be STAR indexing. The first process launched won't be the alignment step, though, because nextflow knows that this step has to be launched after the alignment stage. In the pipeline's parameters, which we will discuss shortly, we have included params.trim and params.align parameters. In the workflow, we use an if statement similar to the following: if (params.align) { ALIGN(input) } Therefore, the users of the pipeline could choose to just do the FastQC stage, or they could skip the trimming stage and do the alignment stage on the raw .fastq files. For understanding and testing the pipeline, we will keep these parameters equal to true , so that we run the entire pipeline every time. If params.trim is true , we run the trimming step on the input_fastq channel, just like we did with the FASTQ process. When we run a process, like TRIM , we can access the files that are emitted by the process using TRIM.out . So, for instance, the TRIM process emits the channel trimmed_fastq , which we can access as follows: TRIM.out.trimmed_fastq . Using this, we can control the flow of data between our processes. Following calling TRIM , we define a set of channels, like so: if (params.trim) { // trim the raw reads and re-apply QC TRIM(input_fastq) // define channels ch_trimmed_fastqc_html = TRIM.out.trimmed_fastqc_html ch_trimmed_fastqc_zip = TRIM.out.trimmed_fastqc_zip ch_trimming_report = TRIM.out.trimming_report ch_fastq_to_align = TRIM.out.trimmed_fastq } else { // if we don't trim, we must align the raw reads ch_fastq_to_align = input_fastq // trimming skipped, define channels as empty for input to multiqc ch_trimmed_fastqc_html = Channel.empty() ch_trimmed_fastqc_zip = Channel.empty() ch_trimming_report = Channel.empty() } This just copies the channels defined by TRIM.out to a set of new variables. Usually, this would not be necessary. We only do it because of the possibility that params.trim is set to false . If it was false , TRIM.out would never be defined. But, we need to send the values of TRIM.out to MultiQC later in the pipeline. If we directly sent the channels defined by TRIM.out to MultiQC, there would be an error if we did not run the trimming step. To get around this, we redirect the output of TRIM.out to a set of new variables, and pass these to MultiQC. Then, if params.trim was false , we can set these channels to empty so that the channels are defined regardless of whether trimming was performed or not. Note also that, if trimming was run, we would set ch_fastq_to_align to be the trimmed .fastq files. These are then used as input to the alignment stage later on. But, if trimming is not run, we instead set this variable to be the raw .fastq files defined by input_fastq . So, regardless of whether trimming was run, ch_fastq_to_align will be a queue channel containing a set of .fastq files to be processed! We next use a similar pattern to run alignment and trimming only if params.align is true . If it is false , we define empty channels to the outputs of these processes that are expected by MultiQC. The first step within the if statement is the indexing step: STAR_INDEX(file(\"$params.genome_fasta\"), file(\"$params.genome_gtf\")) To this channel, we use the file function to provide two inputs. This function creates a value channel from the path of a single file, which is defined within params in this case. The indexing step requires the .fasta and .gtf files to create the genome index. Alternatively, wee could have assigned each of these value channels to a variable, then passed these variables to STAR_INDEX . We then provide the indexed genome created by this process to ALIGN , which creates a .bam file that details how the reads were aligned to the genome. The program htseq-count uses the .bam alignment file produced by ALIGN and the .gft annotations file produced by STAR_INDEX to create the final .counts file that we will use in our downstream analysis. ALIGN( ch_fastq_to_align, STAR_INDEX.out.indexed_genome ) COUNT( ALIGN.out.aligned_bam, STAR_INDEX.out.annotation ) The final step is to run MultiQC on the outputs of FastQC, Trim Galore, STAR and htseq-count. At this stage, the nextflow pipeline differs from the previous pipelines. Previously, we ran MultiQC on the FastQC output, then we ran it on the Trim Galore output, then we ran it on the output of STAR and htseq-count. In the nextflow pipeline, we run MultiQC once at the end of the pipeline. This is because users can specify params.trim = false and/or params.align = false . These outputs are only included in the MultiQC report if they were actually carried out by the pipeline. So, depending on the input parameters, the pipeline can create these same three MultiQC reports. As a first step, we recommend trying to run the pipeline on the test dataset on your local machine. This should be easy to accomplish. As long as you have downloaded nextflow and either of conda or docker, you don't even need to have downloaded this repository to run the pipeline! If you run the following code, nextflow will automatically download the repository and run the pipeline for the test dataset using docker! nextflow run ImperialCollegeLondon/ReCoDE_rnaseq_pipeline -profile test,docker You can replace the \"docker\" profile for the \"conda\" profile if you prefer conda. Alternatively, if you don't want nextflow to re-download the repository, you can run the following: nextflow main.nf -profile test,docker If you have been using the cluster to run everything so far, you could instead use the above command to run nextflow within a PBS job. The main.nf file located in the top-level directory of the exemplar tells nextflow how to run the pipeline. It simply runs the PROCESS_RNASEQ workflow we discussed in this section. Next, we will discuss the config files that are used to configure the pipeline (including defining variables such as params ) before trying to run the pipeline on the cluster! Config files So far, we have glossed over variables such as param that define how our pipeline runs. nextflow pipelines can be configured in a few different ways, including by passing variables via the command line. We will focus on the use of config files to define the behaviour of the pipeline. The main configuration file can be found in the top-level directory by the name of nextflow.config . In this file, we define default parameters for all of our params and set up the \"profiles\" like \"test\" and \"docker\". The first section of the file defines all of our parameters. If we were to use the code below, we would define a single parameter, example_variable , who has a value of one. params { example_variable = 1 } In our actual config file, we define a few different options that change how our pipeline runs. For instance, it contains the variables align and trim that determine which sections of the pipeline are run. It also contains variables that let us change the dataset we want to process. Later in the config, we define profiles . The miniature profiles section below defines a single profile, \"conda\", that activates conda in our processes when it is used. So, we would add the command line argument -profiles conda to run the pipeline using conda. The file contains another profile for docker, which we used to run the pipeline in the previous section. profiles { conda { params.enable_conda = true } } Also in this configuration file is a manifest, which includes metadata relevant to our pipeline. The file also includes the following line: includeConfig 'conf/base.config' This includes all of the configuration options in conf/base.config by default. We use the base configuration file, located in conf , to define default process settings. We relegate this to another file to demonstrate a basic set of process settings. It is likely that the user will want to overwrite these, because their data may need different resources to the default test dataset we have been using. In this file, we define the default number of cores ( cpus ), amount of memory ( memory ) and running time ( time ) that each process in modules/ will be allocated by nextflow. This is similar to how we configured our parallelised pipeline on the cluster! Another important part of the process configuration is the label-based configurations, that start with withLabel . This lets us allocate different resources to each of our processes. All of the processes in modules/ were given a label that corresponds to one of these resource options. To run FastQC, we used the short option, trimming uses the long option, and there are special configurations for the STAR processes, which require lots of memory. Another benefit of nextflow is that it can automatically re-run our jobs when they fail. This is useful if, for instance, STAR used more memory than expected. If this is the case, nextflow can re-run the job automatically with more memory, preventing a total pipeline failure. We set the default maxRetries to 1 and used errorStrategy to tell nextflow only to rerun processes for certain types of errors (e.g. reached max runtime, out of memory). You may have noticed that the label-based configurations use a function called check_max . This is a function that makes sure that the resources used do not exceed the maximum values that have been set in nextflow.config . This function is defined at the bottom of nextflow.config . In nextflow.config we also created a profile called \"test\". In the previous section, you may have managed to run the test dataset on your local machine using this profile. In the profiles section of nextflow.config , we load the conf/test.config options if the \"test\" profile is activated. You can see that this configuration overwrites some of the default parameters in order to run the test dataset. For instance, it specifies that we want to use the smaller test dataset. And, it reduces the maximum available resources so that it can run on a standard computer. In the next section, we will get the pipeline running on the cluster for the full dataset. To do this, we will use the \"imperial_rcs\" profile, which configures nextflow to communicate with the PBS scheduler. Running the pipeline on the computing cluster Finally, we are almost read to run the full nextflow pipeline on a proper RNA-seq dataset! In this section, we will assume that you have downloaded the dataset on the cluster, as per the instructions in docs/parallelised_pipeline.md or data/README.md . Running the pipeline will lead to the generation of results into your home directory, unless you specify otherwise. The intermediate files produced by nextflow (i.e. the work/ directory) will be stored in the temporary Ephemeral space. We will also assume that you set up the recode_rnaseq conda environment for the parallelised pipeline. We first need to configure the pipeline on the cluster. The nextflow.config file contains a profile called \"imperial_rcs\" that, if activated, will include the conf/imperial_rcs.config configuration. This includes some key configuration options for working with the Imperial cluster. In params we set the results folder to a new folder in our home directory. We also set the paths at which nextflow should expect to find the full dataset. The parameters assume you have downloaded this repository to the cluster and stored the data within the data/ directory. If this is not the case, you will need to modify this configuration or create your own. We re-define the process options that we originally setup in base.config . The aim of this is to target specific resources on the Imperial computing cluster. For instance, at time of writing, the STAR indexing and alignment processes target the \"short8\" resource. We also additionally specify the executor . Setting the executor tells nextflow how it is supposed to run each step of the pipeline. If we set the executor to \"pbspro\", it will attempt to submit each step as a job to the PBS scheduler. The default executor, local, is designed for running pipelines locally on a machine with multiple cores. We still use it for the \"short\" and \"long\" jobs, because these jobs are less computationally intensive and can be run from the original nextflow job. We next use an executor block to specify the number of jobs that can be sent to the PBS scheduler at a time by nextflow. Currently, you can't run more than 50 jobs at a time on the Imperial cluster. For this pipeline, we set queueSize to 7 because there are very few samples we are going to run. Finally, we activate singularity. Singularity is similar to docker, in that it can run containers. In fact, singularity can read and run the docker containers we specified earlier. We use singularity on the cluster instead of docker because it is more compatible with the Imperial cluster. For the purposes of running our pipeline on the cluster, we can consider them to be synonymous. We have to specify autoMounts and provide the location of our home and temporary spaces on the cluster using the -B flag. This makes sure that these folders are accessible within our singularity containers. Now that we have our profile set up, we can run nextflow. Our last task is to run nextflow itself as a job. nextflow needs to be run as its own job to coordinate the submission of the pipeline steps to the PBS. For our pipeline, we will also run the processes labelled with \"short\" or \"long\" within this coordinator job, purely so that we don't submit too many smaller jobs to the scheduler. These will run within the coordinator job since we have set their executor to \"local\". You run the following command to start running the nextflow job on the cluster: qsub nextflow_job.pbs We give the coordinator job 4 cores and 16GB, so that nextflow can be run at the same time as the \"local\" jobs. The job activates the recode_rnaseq conda environment we setup for the parallelised pipeline, as it contains nextflow. We export the following environment variables: NXF_OPTS - This ensures Java runs properly on the Imperial cluster. NXF_TEMP - This sets nextflow's temporary directory to the cluster's ephemeral space. NXF_WORK - This sets the work directory to the cluster's ephemeral space. NXF_SINGULARITY_CACHEDIR - This allows singularity to create an image cache in the ephemeral space. Finally, the pipeline is run to completion. If you have problems running the pipeline, you could try running the test dataset instead to work out what the problem is. Failing this, you could submit the problem as an issue . Continous integration Previously, we tested the pipeline locally before we ran it for the full dataset on the computing cluster. Testing is a great way to ensure that you don't inadvertently break something when you update your code. To take testing another step further, we can set up our tests to run automatically every time we make updates to our code; this practice is known as continous integration. One method of setting up continous integration is using GitHub actions. Every time the project code is updated, we update the project on GitHub using a program called git. The main benefit of this process is that git/GitHub track every single change that is made to a project. This allows you to revert things when they go wrong, and you can keep track of all the changes that have been made over time. We can additionally set up a GitHub Actions workflow that will run every time we push changes to GitHub. We have set one such workflow up at .github/workflows/test_pipeline.yml . Including such a file into the .github/workflows directory in your repository will be automatically detected by GitHub Actions. It provides instructions on the jobs we want Actions to carry out. For a full explanation of GitHub actions, check out the documentation provided by GitHub . In our workflow, we specify on: [push, pull_request] to tell Actions that we want the workflow to be run every time we push code to the repository. It will also activate when a pull request is made; pull requests are used to merge new changes into the main project. We then specify a single job, run_pipeline, with the name \"Run pipeline with test data\". Within this job, we designate a strategy in which we create a matrix of parameters describing all the different parameters we want testing. In this case, we have set the job to use ubuntu with the docker profile. We could have included a test on windows by setting os: to [ ubuntu-latest, windows-latest ] , or, if we wanted to test conda as well, we could have set profile: to [ docker, conda ] . This parameter matrix creates a variable called matrix that contains the variables of each job. In our case, we only create one job with one set of parameters for our workflow. We indicate the operating system the workflow that will run on using the variable matrix.os , and, finally, we include a list of steps that will be run as part of the job. These steps are as follows: Checkout pipeline - This step uses a pre-made set of instructions available in another project on GitHub . It checks out the pipeline and clones it into the current working directory. Install nextflow - We download and install nextflow so that we can test the pipeline in a later step. We also set the environment variable NXF_VER to use a specific version of the pipeline. Make scripts executable - By default, the scripts in bin/ are not executable, meaning they cannot be run within the actions environment. We use chmod to ensure the pipeline can run. Run pipeline - Finally, we run the pipeline with the test dataset. We set the profile to docker using the matrix.profile variable. If nextflow returns an error, for instance if an expected file was not generated, then the workflow will fail. This is useful, because it indicates the code or dependencies have changed in a way that breaks the pipeline. You can go to the actions tab on GitHub to view the runs that have been carried out for each repository push. If you scroll down far enough, you might find runs that failed when the pipeline and tests were incomplete! In this exemplar, we created only a simple workflow for doing a full pipeline test. For more complex pipelines, this may not be sufficient. When pipelines grow, it might be necessary to include separate tests for each segment of the pipeline. This is referred to as unit testing. Limitations The pipeline is much improved since our first iteration! We can now run the pipeline on the computing cluster, and we can run the processing for each sample in parallel. Furthermore, using nextflow made the pipeline far more robust, because nextflow will handle the submission and tracking of cluster jobs and it will check that all of the expected outputs are created. One limitation of using nextflow is that it is slower to setup than simply creating some bash scripts as PBS jobs. But, it probably is a lot faster than manually creating robust pipelines that are both parallelised on the cluster and robust to common errors and failures. Another limitation of the pipeline we have created is that it is highly specific to the soybean dataset we are processing. For instance, our pipeline cannot process datasets that use paired end sequencing - this is a very common type of RNA-seq data! Our pipeline also doesn't allow a huge amount of flexibility. We only provide a few basic options in our configuration files. We have created an intentionally simple pipeline to demonstrate the concepts of using computing clusters and building nextflow pipelines. If you have some RNA-seq data you want to process with nextflow, we suggest using the nf-core pipeline which is a highly robust and customisable pipeline. If you wish to gain a better understanding of the pipeline demonstrated in this exemplar, you could try to extend its functionality. You could try to pick a function that the nf-core pipeline has implemented, or choose one of the following: Extend the pipeline to be able to process both single- (like our dataset) and paired-end datasets. Pick a dataset, for instance from the sequence read archive, and try to create an extension of the pipeline to process it. Hint: look into channel.fromFilePairs ! Add a new tool to the pipeline. For instance, could swap out htseq-count for another tool like featureCounts. Make the pipeline more customisable. Look up the arguments for one of the programs we have used and try to allow users of the pipeline to change this parameter using a .config file. Bonus: The current continous integration tests are very basic. Create a more complex testing procedure that ensures each part of the pipeline is still running correctly. Nextflow's test repository might be helpful for this. For the creation of this pipeline, we used the program git to keep track of all the changes we made along the way. We then uploaded the project to GitHub so it can be shared with others. If you are interested in learning this too, you could try using git to keep track of the updates you make. The easiest way to do this is to go to the exemplar's GitHub repository and click the \"Fork\" button. You can now clone your forked repository and begin updating it. Now, when you make changes, you can push them to your forked repository on GitHub.","title":"Nextflow Pipeline - Running"},{"location":"running_nextflow_pipeline/#running-the-nextflow-pipeline","text":"","title":"Running the nextflow pipeline"},{"location":"running_nextflow_pipeline/#set-up","text":"Previously, we created a single conda environment in which we installed all of the packages we needed to run the pipeline. For the nextflow pipeline, we will instead create an environment for each step of the pipeline. By this, we mean that we will separate each step (QC, trimming, indexing, alignment, counting, multiqc) and use an environment that contains only the package we need. For instance, in the QC step we will create an environment that contains only FastQC and its dependencies. Using different environments for each step is useful as the project gets bigger, so that we don't have to create huge and complex conda environments. As we will see when we create the nextflow pipeline, we don't need to set up these environments manually; nextflow will create the environments for us! conda integration is a relatively new feature to nextflow, and it is generally recommended to user docker containers instead. So, we need to make a container for each of the pipeline steps that contains the applications we need. nextflow is helpful in that it will download and activate the containers we specify automatically, but these containers must exist to begin with! Luckily for us, containers have already been created for many common bioinformatics tools. They are created and hosted by the BioContainers project . Try visiting their website and finding a container image for the FastQC program. When we write our pipeline we will show you how to specify the container image that contains the relevant packages. First, though, we will discuss how to create your own containers. We have already found a FastQC container image that we could use, but for the sake of learning we will try and create our own FastQC container. Containers are stored as \"images\", which are instructions on how to assemble a docker container. We can create our own image that contains FastQC, and use this to build our environment as part of our pipeline. After we have downloaded docker and created a dockerfile (you can find ours in the top-level directory of the exemplar), we can begin specifying our image. To do this, we will be creating an image that is similar to the BioContainers FastQC image. The first step in creating the image is to choose a base image. The base image is the docker image you will build your own image on top of. For instance, you may choose to use a minimal linux image to build upon. We will use the base biocontainers image, which uses Ubuntu as its operating system and includes conda within the container. We do this like so: FROM biocontainers/biocontainers:v1.1.0_cv2 Below, we use USER root to switch to the root user, because we need admin permissions to download and setup FastQC. We then download FastQC and its dependency java, unpack it and allow it to be executed. USER root RUN apt-get update && apt-get install -y openjdk-8-jre-headless RUN mkdir -p /opt/fastqc RUN wget https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.9.zip --no-check-certificate -O /opt/fastqc/fastqc_v0.11.9.zip RUN unzip /opt/fastqc/fastqc_v0.11.9.zip -d /opt/fastqc RUN rm /opt/fastqc/fastqc_v0.11.9.zip RUN mv /opt/fastqc/FastQC/* /opt/fastqc RUN rmdir /opt/fastqc/FastQC && chmod +x /opt/fastqc/fastqc Next, we create a link to the program in /usr/local/bin/ and add the directory to our path. Now, when we run the fastqc command within our container, it will run the program. RUN ln -s /opt/fastqc/fastqc /usr/local/bin/fastqc ENV PATH /usr/local/bin:$PATH We would prefer that the container be run with a non-root user. This is considered best practice for security: why run FastQC as an admin when we can do it as a standard user? So, we create and switch to a non-root user for when we actually run the container. RUN adduser --system --no-create-home nonroot USER nonroot To use our docker container, we first have to build it as an image. To do this, we will use the docker build command. For this exemplar, I created an account on dockerhub, which allows users to host and share docker images, and stored the image on dockerhub. So, after building the image, I additionally pushed the image to dockerhub, as so: # docker build -t jackgisby/fastqc . # docker push jackgisby/fastqc This way, you can use my docker image or edit the pipeline to use the image you have created. Usually our next step would be to demonstrate how to access and use the docker image directly, but nextflow will take care of this for us, as we will demonstrate in the following sections. If you want to use docker in your other work, it may be useful to read the docker documentation or a specific course on docker.","title":"Set Up"},{"location":"running_nextflow_pipeline/#running-the-pipeline","text":"We can now start building our nextflow pipeline. We will create a module for each of our processing steps in bin/ and connect them together into a cohesive nextflow pipeline. We will allow users to use either docker or conda to run the pipeline. We will test the pipeline locally before running it on the cluster for the full dataset. Finally, we will look at how we can set up GitHub actions to test the pipeline every time we make a change. We will go over the main features of our workflow, but we will not spend much time providing background to nextflow. It may be useful to consult other resources, like the nextflow documentation , to find out more details or clarify how elements of the pipeline work.","title":"Running the pipeline"},{"location":"running_nextflow_pipeline/#building-the-pipeline-processes","text":"The first step in creating our pipeline will be to create a nextflow process to run each of the steps in bin/ . We will create these in the modules/ directory. For instance, a minimal process for running FastQC on a .fastq file might look like this: process FASTQC { input: path fastq output: path \"*_fastqc.html\", emit: fastqc_html path \"*_fastqc.zip\", emit: fastqc_zip script: \"\"\" fastqc -o ./ ./$fastq \"\"\" } The key parts of this process definition include: input - The inputs are the files that are going to be used as input to this process. In this case we want to process a .fastq file, so we specify that we expect a path as input, and we name this \"fastq\". output - The outputs are the files that we expect our program is going to generate. We don't need to specify all of the program's outputs, just the ones that we want to keep. In this case, we want to capture the files that end in \"_fastqc.html\" and those that end in \"_fastqc.zip\"; we achieve this using the wildcard character, \"*\". These files represent the FastQC report that we will later pass to MultiQC in order to view the QC metrics for our .fastq files. We use emit: to name the outputs to make accessing them later easier. The \"*\" wildcard symbol is used, which will match any text. script - The script contains one or more commands to be run as part of the process. This is like a bash script, and it can access the variables we defined above in the process. For instance, we use the fastq variable to access the location of the .fastq file that needs to be processed. This script will run fastqc on this .fastq file and save the results to the working directory. We will discuss later how to pass the inputs and recover the outputs from these processes. You might notice that the actual module, modules/fastqc.nf is slightly more complex, demonstrated below: process FASTQC { label \"long\" publishDir \"$params.outdir/a_fastqc/\", mode: params.publish_dir_mode conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null) // use a custom container rather than biocontainers // container \"quay.io/biocontainers/fastqc:0.11.9--0\" container \"jackgisby/fastqc:v1\" input: tuple val(accession), path(fastq) output: path \"*_fastqc.html\", emit: fastqc_html path \"*_fastqc.zip\", emit: fastqc_zip script: \"\"\" mkdir fastqc_tmp $baseDir/bin/fastqc.sh \\ \"./\" \\ \"$fastq\" \\ \"./fastqc_tmp\" \"\"\" } One major addition is that our process now accesses variables stored within params . This variable contains parameters that let us modify how our pipeline runs. We will discuss how we can set and modify these parameters later. The key additions to the module are as follows: label - We can add our processes to different groups using labels. We will make use of labels later when we use them to decide which cluster resources we will allocate to each process. publishDir - Even though we have specified expected outputs, nextflow does not necessarily make it easy for us to retrieve our results. nextflow stores all of the input and output files into a directory named work/ . This is where nextflow does its processing, and it makes it easier for nextflow to isolate each of the processes when they may be running concurrently. If we specify publishDir , nextflow will copy the expected output files to a directory of our choosing. We specify the directory as $params.outdir/a_fastqc/ - i.e. we want the results saved to the a_fastqc directory within the folder specified by $params.outdir . We set the mode variable which controls the method nextflow uses to make the results available. conda - If we are using conda to get the FastQC program, then this tells nextflow where it can find FastQC. The variable params.enable_conda variable indicates whether we are using conda. If so, we tell nextflow to use a specific version of FastQC available from Bioconda (\"bioconda::fastqc=0.11.9\"), else we use null to indicate nextflow should not download a conda environment. container - If we are not using conda, we are going to use containers instead. This command specifies which container nextflow should set up to run the analysis. To do this, we could use the container \"quay.io/biocontainers/fastqc:0.11.9--0\", which is the URL of the biocontainers FastQC image. But, for this process, we will instead use the docker image we created earlier: \"jackgisby/fastqc:v1\". This will get version 1 of the FastQC container I published to dockerhub. input - We changed the input so that it accepts both the path of the .fastq file, but also its sample ID. This might be useful for naming the outputs using the variable accession . script - Instead of running FastQC directly, we are now using the script bin/fastqc.sh instead. In general I don't recommend doing this, but we will do it this way for consistency with the previous iterations of the pipeline that we developed! Remember that nextflow can also run other types of scripts - in this section we could alternatively run a Python or an R script. Note that we use the baseDir variable: this is the base directory of the nextflow pipeline (i.e. the top-level directory of this repository). We have created similar processes in the modules/ directory for each of our pipeline steps. Many of these are very similar, we just run the relevant script in bin/ while specifying the input and output files that are specific to the program that has been run. Next we will move on to specifying the sequence of the analysis, so it might be worthwhile to look at the other processes and try and understand what is going on. You should be able to match up the inputs and outputs to the files we generated for the previous pipelines.","title":"Building the pipeline processes"},{"location":"running_nextflow_pipeline/#putting-the-pipeline-together","text":"We will now create a script, workflows/nextflow_pipeline.nf , to create a cohesive workflow. This will simply join the processes we have created into a single script. With nextflow, we don't explicitly state the order tasks should be run. We simply define how the data flows between each of our processes, and nextflow will decide on the running order and will run steps in parallel where necessary! The first part of our workflow script simply imports the processes that we generaed previously, for example: include { FASTQC } from \"$baseDir/modules/fastqc.nf\" Instead of defining a process, we now define a workflow. Workflows are used for orchestrating the flow of data between multiple processes. A more complex project might even define workflows within other workflows, but our use case is a lot simpler. We define a workflow called PROCESS_RNASEQ that will represent our end-to-end pipeline. workflow PROCESS_RNASEQ { } Within this workflow, we start by getting the input data. nextflow uses variables, called \"channels\", to describe how data will flow through a workflow. There are two different types of channels. \"Value\" channels are like regular variables we would define in bash, Python or R. It contains a single value and can be passed to nextflow processes to change how they run. The other type of channel is a \"Queue\" channel. These channels typically contain multiple inputs that each need to be processed. There are multiple ways of defining these channels, but we will use fromPath . This allows us to create a channel from a set of input files, like our .fastq files. To create our queue, we define a channel called input_fastq using Channel.fromPath with the .fastq files as input. We specify that the channel should look into the params.fastqc_dir directory and include any files within it that end with \".fastq.gz\" using the \"*\" wildcard character. This on its own is enough to setup a channel containing all of the .fastq files. We do an additional step, using the .map command, that adds the name of the sample to the channel. In the process modules, this lets us get both the sample name and path. input_fastq = Channel.fromPath(\"${params.fastqc_dir}/*.fastq.gz\").map { tuple( it.name.split('\\\\.')[0], it ) } In the next step, we use the FASTQC as if it was a function by running: FASTQC(input_fastq) . nextflow understands that, since there are multiple files in the input_fastq channel, it must run the FASTQC process for each element of the channel. This is the behaviour we want, because we need to run fastqc on each sample individually. Note that, just because we call the FASTQC process first, doesn't mean this will be the first thing nextflow runs. For instance, the first process launched might be STAR indexing. The first process launched won't be the alignment step, though, because nextflow knows that this step has to be launched after the alignment stage. In the pipeline's parameters, which we will discuss shortly, we have included params.trim and params.align parameters. In the workflow, we use an if statement similar to the following: if (params.align) { ALIGN(input) } Therefore, the users of the pipeline could choose to just do the FastQC stage, or they could skip the trimming stage and do the alignment stage on the raw .fastq files. For understanding and testing the pipeline, we will keep these parameters equal to true , so that we run the entire pipeline every time. If params.trim is true , we run the trimming step on the input_fastq channel, just like we did with the FASTQ process. When we run a process, like TRIM , we can access the files that are emitted by the process using TRIM.out . So, for instance, the TRIM process emits the channel trimmed_fastq , which we can access as follows: TRIM.out.trimmed_fastq . Using this, we can control the flow of data between our processes. Following calling TRIM , we define a set of channels, like so: if (params.trim) { // trim the raw reads and re-apply QC TRIM(input_fastq) // define channels ch_trimmed_fastqc_html = TRIM.out.trimmed_fastqc_html ch_trimmed_fastqc_zip = TRIM.out.trimmed_fastqc_zip ch_trimming_report = TRIM.out.trimming_report ch_fastq_to_align = TRIM.out.trimmed_fastq } else { // if we don't trim, we must align the raw reads ch_fastq_to_align = input_fastq // trimming skipped, define channels as empty for input to multiqc ch_trimmed_fastqc_html = Channel.empty() ch_trimmed_fastqc_zip = Channel.empty() ch_trimming_report = Channel.empty() } This just copies the channels defined by TRIM.out to a set of new variables. Usually, this would not be necessary. We only do it because of the possibility that params.trim is set to false . If it was false , TRIM.out would never be defined. But, we need to send the values of TRIM.out to MultiQC later in the pipeline. If we directly sent the channels defined by TRIM.out to MultiQC, there would be an error if we did not run the trimming step. To get around this, we redirect the output of TRIM.out to a set of new variables, and pass these to MultiQC. Then, if params.trim was false , we can set these channels to empty so that the channels are defined regardless of whether trimming was performed or not. Note also that, if trimming was run, we would set ch_fastq_to_align to be the trimmed .fastq files. These are then used as input to the alignment stage later on. But, if trimming is not run, we instead set this variable to be the raw .fastq files defined by input_fastq . So, regardless of whether trimming was run, ch_fastq_to_align will be a queue channel containing a set of .fastq files to be processed! We next use a similar pattern to run alignment and trimming only if params.align is true . If it is false , we define empty channels to the outputs of these processes that are expected by MultiQC. The first step within the if statement is the indexing step: STAR_INDEX(file(\"$params.genome_fasta\"), file(\"$params.genome_gtf\")) To this channel, we use the file function to provide two inputs. This function creates a value channel from the path of a single file, which is defined within params in this case. The indexing step requires the .fasta and .gtf files to create the genome index. Alternatively, wee could have assigned each of these value channels to a variable, then passed these variables to STAR_INDEX . We then provide the indexed genome created by this process to ALIGN , which creates a .bam file that details how the reads were aligned to the genome. The program htseq-count uses the .bam alignment file produced by ALIGN and the .gft annotations file produced by STAR_INDEX to create the final .counts file that we will use in our downstream analysis. ALIGN( ch_fastq_to_align, STAR_INDEX.out.indexed_genome ) COUNT( ALIGN.out.aligned_bam, STAR_INDEX.out.annotation ) The final step is to run MultiQC on the outputs of FastQC, Trim Galore, STAR and htseq-count. At this stage, the nextflow pipeline differs from the previous pipelines. Previously, we ran MultiQC on the FastQC output, then we ran it on the Trim Galore output, then we ran it on the output of STAR and htseq-count. In the nextflow pipeline, we run MultiQC once at the end of the pipeline. This is because users can specify params.trim = false and/or params.align = false . These outputs are only included in the MultiQC report if they were actually carried out by the pipeline. So, depending on the input parameters, the pipeline can create these same three MultiQC reports. As a first step, we recommend trying to run the pipeline on the test dataset on your local machine. This should be easy to accomplish. As long as you have downloaded nextflow and either of conda or docker, you don't even need to have downloaded this repository to run the pipeline! If you run the following code, nextflow will automatically download the repository and run the pipeline for the test dataset using docker! nextflow run ImperialCollegeLondon/ReCoDE_rnaseq_pipeline -profile test,docker You can replace the \"docker\" profile for the \"conda\" profile if you prefer conda. Alternatively, if you don't want nextflow to re-download the repository, you can run the following: nextflow main.nf -profile test,docker If you have been using the cluster to run everything so far, you could instead use the above command to run nextflow within a PBS job. The main.nf file located in the top-level directory of the exemplar tells nextflow how to run the pipeline. It simply runs the PROCESS_RNASEQ workflow we discussed in this section. Next, we will discuss the config files that are used to configure the pipeline (including defining variables such as params ) before trying to run the pipeline on the cluster!","title":"Putting the pipeline together"},{"location":"running_nextflow_pipeline/#config-files","text":"So far, we have glossed over variables such as param that define how our pipeline runs. nextflow pipelines can be configured in a few different ways, including by passing variables via the command line. We will focus on the use of config files to define the behaviour of the pipeline. The main configuration file can be found in the top-level directory by the name of nextflow.config . In this file, we define default parameters for all of our params and set up the \"profiles\" like \"test\" and \"docker\". The first section of the file defines all of our parameters. If we were to use the code below, we would define a single parameter, example_variable , who has a value of one. params { example_variable = 1 } In our actual config file, we define a few different options that change how our pipeline runs. For instance, it contains the variables align and trim that determine which sections of the pipeline are run. It also contains variables that let us change the dataset we want to process. Later in the config, we define profiles . The miniature profiles section below defines a single profile, \"conda\", that activates conda in our processes when it is used. So, we would add the command line argument -profiles conda to run the pipeline using conda. The file contains another profile for docker, which we used to run the pipeline in the previous section. profiles { conda { params.enable_conda = true } } Also in this configuration file is a manifest, which includes metadata relevant to our pipeline. The file also includes the following line: includeConfig 'conf/base.config' This includes all of the configuration options in conf/base.config by default. We use the base configuration file, located in conf , to define default process settings. We relegate this to another file to demonstrate a basic set of process settings. It is likely that the user will want to overwrite these, because their data may need different resources to the default test dataset we have been using. In this file, we define the default number of cores ( cpus ), amount of memory ( memory ) and running time ( time ) that each process in modules/ will be allocated by nextflow. This is similar to how we configured our parallelised pipeline on the cluster! Another important part of the process configuration is the label-based configurations, that start with withLabel . This lets us allocate different resources to each of our processes. All of the processes in modules/ were given a label that corresponds to one of these resource options. To run FastQC, we used the short option, trimming uses the long option, and there are special configurations for the STAR processes, which require lots of memory. Another benefit of nextflow is that it can automatically re-run our jobs when they fail. This is useful if, for instance, STAR used more memory than expected. If this is the case, nextflow can re-run the job automatically with more memory, preventing a total pipeline failure. We set the default maxRetries to 1 and used errorStrategy to tell nextflow only to rerun processes for certain types of errors (e.g. reached max runtime, out of memory). You may have noticed that the label-based configurations use a function called check_max . This is a function that makes sure that the resources used do not exceed the maximum values that have been set in nextflow.config . This function is defined at the bottom of nextflow.config . In nextflow.config we also created a profile called \"test\". In the previous section, you may have managed to run the test dataset on your local machine using this profile. In the profiles section of nextflow.config , we load the conf/test.config options if the \"test\" profile is activated. You can see that this configuration overwrites some of the default parameters in order to run the test dataset. For instance, it specifies that we want to use the smaller test dataset. And, it reduces the maximum available resources so that it can run on a standard computer. In the next section, we will get the pipeline running on the cluster for the full dataset. To do this, we will use the \"imperial_rcs\" profile, which configures nextflow to communicate with the PBS scheduler.","title":"Config files"},{"location":"running_nextflow_pipeline/#running-the-pipeline-on-the-computing-cluster","text":"Finally, we are almost read to run the full nextflow pipeline on a proper RNA-seq dataset! In this section, we will assume that you have downloaded the dataset on the cluster, as per the instructions in docs/parallelised_pipeline.md or data/README.md . Running the pipeline will lead to the generation of results into your home directory, unless you specify otherwise. The intermediate files produced by nextflow (i.e. the work/ directory) will be stored in the temporary Ephemeral space. We will also assume that you set up the recode_rnaseq conda environment for the parallelised pipeline. We first need to configure the pipeline on the cluster. The nextflow.config file contains a profile called \"imperial_rcs\" that, if activated, will include the conf/imperial_rcs.config configuration. This includes some key configuration options for working with the Imperial cluster. In params we set the results folder to a new folder in our home directory. We also set the paths at which nextflow should expect to find the full dataset. The parameters assume you have downloaded this repository to the cluster and stored the data within the data/ directory. If this is not the case, you will need to modify this configuration or create your own. We re-define the process options that we originally setup in base.config . The aim of this is to target specific resources on the Imperial computing cluster. For instance, at time of writing, the STAR indexing and alignment processes target the \"short8\" resource. We also additionally specify the executor . Setting the executor tells nextflow how it is supposed to run each step of the pipeline. If we set the executor to \"pbspro\", it will attempt to submit each step as a job to the PBS scheduler. The default executor, local, is designed for running pipelines locally on a machine with multiple cores. We still use it for the \"short\" and \"long\" jobs, because these jobs are less computationally intensive and can be run from the original nextflow job. We next use an executor block to specify the number of jobs that can be sent to the PBS scheduler at a time by nextflow. Currently, you can't run more than 50 jobs at a time on the Imperial cluster. For this pipeline, we set queueSize to 7 because there are very few samples we are going to run. Finally, we activate singularity. Singularity is similar to docker, in that it can run containers. In fact, singularity can read and run the docker containers we specified earlier. We use singularity on the cluster instead of docker because it is more compatible with the Imperial cluster. For the purposes of running our pipeline on the cluster, we can consider them to be synonymous. We have to specify autoMounts and provide the location of our home and temporary spaces on the cluster using the -B flag. This makes sure that these folders are accessible within our singularity containers. Now that we have our profile set up, we can run nextflow. Our last task is to run nextflow itself as a job. nextflow needs to be run as its own job to coordinate the submission of the pipeline steps to the PBS. For our pipeline, we will also run the processes labelled with \"short\" or \"long\" within this coordinator job, purely so that we don't submit too many smaller jobs to the scheduler. These will run within the coordinator job since we have set their executor to \"local\". You run the following command to start running the nextflow job on the cluster: qsub nextflow_job.pbs We give the coordinator job 4 cores and 16GB, so that nextflow can be run at the same time as the \"local\" jobs. The job activates the recode_rnaseq conda environment we setup for the parallelised pipeline, as it contains nextflow. We export the following environment variables: NXF_OPTS - This ensures Java runs properly on the Imperial cluster. NXF_TEMP - This sets nextflow's temporary directory to the cluster's ephemeral space. NXF_WORK - This sets the work directory to the cluster's ephemeral space. NXF_SINGULARITY_CACHEDIR - This allows singularity to create an image cache in the ephemeral space. Finally, the pipeline is run to completion. If you have problems running the pipeline, you could try running the test dataset instead to work out what the problem is. Failing this, you could submit the problem as an issue .","title":"Running the pipeline on the computing cluster"},{"location":"running_nextflow_pipeline/#continous-integration","text":"Previously, we tested the pipeline locally before we ran it for the full dataset on the computing cluster. Testing is a great way to ensure that you don't inadvertently break something when you update your code. To take testing another step further, we can set up our tests to run automatically every time we make updates to our code; this practice is known as continous integration. One method of setting up continous integration is using GitHub actions. Every time the project code is updated, we update the project on GitHub using a program called git. The main benefit of this process is that git/GitHub track every single change that is made to a project. This allows you to revert things when they go wrong, and you can keep track of all the changes that have been made over time. We can additionally set up a GitHub Actions workflow that will run every time we push changes to GitHub. We have set one such workflow up at .github/workflows/test_pipeline.yml . Including such a file into the .github/workflows directory in your repository will be automatically detected by GitHub Actions. It provides instructions on the jobs we want Actions to carry out. For a full explanation of GitHub actions, check out the documentation provided by GitHub . In our workflow, we specify on: [push, pull_request] to tell Actions that we want the workflow to be run every time we push code to the repository. It will also activate when a pull request is made; pull requests are used to merge new changes into the main project. We then specify a single job, run_pipeline, with the name \"Run pipeline with test data\". Within this job, we designate a strategy in which we create a matrix of parameters describing all the different parameters we want testing. In this case, we have set the job to use ubuntu with the docker profile. We could have included a test on windows by setting os: to [ ubuntu-latest, windows-latest ] , or, if we wanted to test conda as well, we could have set profile: to [ docker, conda ] . This parameter matrix creates a variable called matrix that contains the variables of each job. In our case, we only create one job with one set of parameters for our workflow. We indicate the operating system the workflow that will run on using the variable matrix.os , and, finally, we include a list of steps that will be run as part of the job. These steps are as follows: Checkout pipeline - This step uses a pre-made set of instructions available in another project on GitHub . It checks out the pipeline and clones it into the current working directory. Install nextflow - We download and install nextflow so that we can test the pipeline in a later step. We also set the environment variable NXF_VER to use a specific version of the pipeline. Make scripts executable - By default, the scripts in bin/ are not executable, meaning they cannot be run within the actions environment. We use chmod to ensure the pipeline can run. Run pipeline - Finally, we run the pipeline with the test dataset. We set the profile to docker using the matrix.profile variable. If nextflow returns an error, for instance if an expected file was not generated, then the workflow will fail. This is useful, because it indicates the code or dependencies have changed in a way that breaks the pipeline. You can go to the actions tab on GitHub to view the runs that have been carried out for each repository push. If you scroll down far enough, you might find runs that failed when the pipeline and tests were incomplete! In this exemplar, we created only a simple workflow for doing a full pipeline test. For more complex pipelines, this may not be sufficient. When pipelines grow, it might be necessary to include separate tests for each segment of the pipeline. This is referred to as unit testing.","title":"Continous integration"},{"location":"running_nextflow_pipeline/#limitations","text":"The pipeline is much improved since our first iteration! We can now run the pipeline on the computing cluster, and we can run the processing for each sample in parallel. Furthermore, using nextflow made the pipeline far more robust, because nextflow will handle the submission and tracking of cluster jobs and it will check that all of the expected outputs are created. One limitation of using nextflow is that it is slower to setup than simply creating some bash scripts as PBS jobs. But, it probably is a lot faster than manually creating robust pipelines that are both parallelised on the cluster and robust to common errors and failures. Another limitation of the pipeline we have created is that it is highly specific to the soybean dataset we are processing. For instance, our pipeline cannot process datasets that use paired end sequencing - this is a very common type of RNA-seq data! Our pipeline also doesn't allow a huge amount of flexibility. We only provide a few basic options in our configuration files. We have created an intentionally simple pipeline to demonstrate the concepts of using computing clusters and building nextflow pipelines. If you have some RNA-seq data you want to process with nextflow, we suggest using the nf-core pipeline which is a highly robust and customisable pipeline. If you wish to gain a better understanding of the pipeline demonstrated in this exemplar, you could try to extend its functionality. You could try to pick a function that the nf-core pipeline has implemented, or choose one of the following: Extend the pipeline to be able to process both single- (like our dataset) and paired-end datasets. Pick a dataset, for instance from the sequence read archive, and try to create an extension of the pipeline to process it. Hint: look into channel.fromFilePairs ! Add a new tool to the pipeline. For instance, could swap out htseq-count for another tool like featureCounts. Make the pipeline more customisable. Look up the arguments for one of the programs we have used and try to allow users of the pipeline to change this parameter using a .config file. Bonus: The current continous integration tests are very basic. Create a more complex testing procedure that ensures each part of the pipeline is still running correctly. Nextflow's test repository might be helpful for this. For the creation of this pipeline, we used the program git to keep track of all the changes we made along the way. We then uploaded the project to GitHub so it can be shared with others. If you are interested in learning this too, you could try using git to keep track of the updates you make. The easiest way to do this is to go to the exemplar's GitHub repository and click the \"Fork\" button. You can now clone your forked repository and begin updating it. Now, when you make changes, you can push them to your forked repository on GitHub.","title":"Limitations"},{"location":"running_parallelised_pipeline/","text":"Running the parallelised pipeline on Imperial computing cluster In docs/simple_local_pipeline.md , we discussed the limitations of running our pipeline as a simple script on a local machine. We noted that running the pipeline on a standard laptop was only feasible for small datasets, because larger datasets would require more memory than we had available. We could fix this by running the pipeline on the cluster instead as a job - this way, we can access as much memory as we may need. So, why do we need to modify the pipeline, beyond just adding resource specifications to the top of the script? We also said that the pipeline was limited because it could only run a single task at a time, so it will be slow when we have large numbers of samples. This is a more difficult problem to solve. Some of our pipeline steps, for instance indexing the genome with STAR and generating reports with MultiQC, only need to be run once. Other steps, like FastQC and STAR alignment, must be run for every single sample. So, we will have to reconfigure our pipeline to run efficiently on the cluster. Set up To run the full pipeline on the cluster, you must have setup the conda environment and downloaded this repository. The instructions to do this are available in this exemplar's top-level README.md . Note, however, that conda works slightly differently on the imperial cluster. You must activate the conda module using the following code: module load anaconda3/personal If you have never run conda before, you must initialise it like so: anaconda-setup Finally, you can install the recode_rnaseq environment: conda update conda conda env create -f environment.yml Note that to run the pipeline and to install the conda environment, you must run the commands from within the ReCoDE_rnaseq_pipeline repository. The code to setup the conda environment is also included in the script pbs/setup_pipeline.pbs , so you could just uncomment those lines and run the pipeline and install the environment in one go. The install commands in this script use mamba , which is a lot faster than conda. So, if you're having trouble installing the environment, try using mamba . You can then run the pipeline either by running the command workflows/parallelised_pipeline.sh from the cluster command line (do not use the qsub command to run this script), or by running the individual lines of code within workflows/parallelised_pipeline.sh . On the Imperial cluster, this should automatically run the pipeline for the test dataset. If you are using a cluster owned by a different institution, you may need to do some work to adapt the scripts to your scheduler. If you have any problems, please raise them as an issue . Adapting the simple pipeline to run on the cluster Our first step is to break the pipeline up into multiple stages. We need to separate the elements of the pipeline that should only be run once, and the parts of the pipeline that need to be run for each sample. Specifically, we need to create the results folders before we start generating results. We also need to create a STAR index before we run the alignment step. So, we create a script, pbs/setup_pipeline.pbs , which runs these stages first. While MultiQC is another step that only needs to be run once, it can't be run in this script because it has to be run last! So, the next step is to carry out the data processing steps for each sample. In workflows/simple_local_pipeline.sh , we used loops to run the QC, trimming, alignment and counting scripts. Since we are now on the cluster, we will instead leverage array jobs to run these steps for each of our samples. We create a script, pbs/parallel_samples.pbs , which is designed to process a single sample. In the workflows/simple_local_pipeline.sh script, we looped through the sample IDs, and in each iteration we assigned the sample ID to the variable s . In pbs/parallel_samples.pbs , we take all the code from within these loops, but we instead define s at the top of the script using the environment variable PBS_ARRAY_INDEX . We will later set up this script to run as an array, such that pbs/parallel_samples.pbs is run six times, once for each sample; each of the six array jobs will have a different number for the variable PBS_ARRAY_INDEX , so that each script has a different value of s . To do this, we get the list of samples names like we did in the simple pipeline, as follows: while IFS=\\= read srr; do SAMPLE_SRR+=($srr) done < data/files.txt Then, we select the Nth value of the sample list (i.e. the Nth row of data/files.txt ), in the following code: s=\"${SAMPLE_SRR[$PBS_ARRAY_INDEX - 1]}\" The script then runs the data processing steps for the corresponding sample. Once all of the samples have been processed, we can finally run MultiQC to collect the reports. This is achieved using the script pbs.multiqc.pbs . We also added the PBS parameters to the top of each of the .pbs scripts. We gave both pbs/setup_pipeline.pbs and parallel_samples.pbs multiple cores and high amounts of memory, because STAR can make use of them. MultiQC is a lot less demanding, so we gave pbs/multiqc.pbs only a single core and 4GB of RAM. These parameters might need modifications for different genomes or larger amounts of data, but for the soybean dataset they are sufficient. Running the pipeline Having now split workflows/simple_local_pipeline.sh into three different stages, we can submit them to the scheduler. The code needed to do this is available in workflows/parallelised_pipeline.sh . This contains the qsub commands we need to submit each of the job to the cluster in order. The first command in this script, below, submits the setup script to the scheduler and stores its job ID to the variable sp_jid . sp_jid=\"$(qsub pbs/setup_pipeline.pbs)\" We next read the samples to be processed as a bash array, as below. We do this to calculate the number of samples to be processed, so that we know how large the array job needs to be. while IFS=\\= read srr; do SAMPLE_SRR+=($srr) done < data/files.txt The most complex command is the submission of the array job: ps_jid=\"$(qsub -W depend=afterok:\"${sp_jid}\" -J 1-\"${#SAMPLE_SRR[@]}\" pbs/parallel_samples.pbs)\" Like we did for the setup script, we submit pbs/parallel_samples.pbs to the scheduler using qsub and save its job ID to the variable ps_jid . Conveniently, all of the array jobs are connected to a single job ID, which we can store to track its progress. The argument -W depend=afterok:\"${sp_jid}\" tells the scheduler not to start the array job until the setup job is complete. Finally, the 1-\"${#SAMPLE_SRR[@]}\" argument sets the script up as an array job with array IDs from 1 to the number of samples in the SAMPLE_SRR array. Finally, we schedule the MultiQC script to run after the array job is complete, like so: mq_jid=\"$(qsub -W depend=afterok:\"${ps_jid}\" pbs/multiqc.pbs)\" Running the pipeline for the full dataset Now we can try and run the pipeline for the full soybean dataset! This is a lot bigger, but now that we are on the cluster it should be simple. The first step is to change the DATA_DIR variable in each of the scripts in the pbs/ directory from \"data/test\" to \"data/\". Then, you will need to download the data! There is a script available for doing this: data/get_data.sh . This will take some time, so you will need to run it as a PBS job. The easiest way to do this is to uncomment the code data/get_data.sh in the pbs/setup_pipeline.pbs . This way you will download the data as part of the setup - you may need to increase the lwalltime parameter so that everything runs though! Note that this script uses a tool called fastq-dump to get the data from the Sequence Read Archive. You could either try installing it using conda, or on the imperial cluster you can use the sra-toolkit module. To do this, just uncomment the line module load sra-toolkit/2.8.1 in the data/get_data.sh script. Now that you have done this, you can run the script like we did for the test data. It will take a bit longer, but not too long because we are only running six samples. The benefit of the parallelised pipeline would be even more obvious if we were to try running hundreds of samples! Once the pipeline has run, you can check the MultiQC outputs and see how they look for a real dataset. If of interest, you could try creating a job to run workflows/simple_local_pipeline.sh for the full dataset. Since we only have six samples, this will run, but it will be a lot slower than the parallelised pipeline! Limitations The parallelised pipeline alleviated some of the limitations of the simple local pipeline. Running the pipeline on the cluster gives us access to the resources we need to run compute-intensive tools like STAR. We also made use of array jobs to run each sample in parallel, making the processing a lot faster. In docs/simple_local_pipeline.md we also noted that the pipeline was prone to failure. What if a file wasn't generated for one of the samples and we never noticed? What if a file was generated but there was an error that prevented the process from finishing? These problems may have been missed by the simple pipeline. And, sadly, they may also be missed by the parallelised pipeline. If the array job fails completely, the job will finish and MultiQC won't run, because it depends on the array job. So, we might pick up major problems with the pipeline. But, there might still be an error for a particular sample that doesn't get picked up. In this case, the parallelised pipeline might be even worse than the simpler one! To make sure each stage ran correctly we would have to check each of the error files produced by each of the array jobs. We could make this pipeline more robust. For instance, we could improve the final script ( pbs/multiqc.pbs ) by adding code that checks the right files were generated for all of the samples. This would make us a lot more confident that the pipeline had run correctly. But, writing all these checks takes time that we could be using to process more data or perform downstream analysis. And, eventually we will write a pipeline and forget to check something important. In the next document, docs/nextflow_pipeline.sh , we will discuss the use of Nextflow to manage our pipeline. Nextflow will allow us to more elegantly tie our pipeline steps together, and it will make sure that every stage produces the expected outputs!","title":"Parallelised Pipeline - Running"},{"location":"running_parallelised_pipeline/#running-the-parallelised-pipeline-on-imperial-computing-cluster","text":"In docs/simple_local_pipeline.md , we discussed the limitations of running our pipeline as a simple script on a local machine. We noted that running the pipeline on a standard laptop was only feasible for small datasets, because larger datasets would require more memory than we had available. We could fix this by running the pipeline on the cluster instead as a job - this way, we can access as much memory as we may need. So, why do we need to modify the pipeline, beyond just adding resource specifications to the top of the script? We also said that the pipeline was limited because it could only run a single task at a time, so it will be slow when we have large numbers of samples. This is a more difficult problem to solve. Some of our pipeline steps, for instance indexing the genome with STAR and generating reports with MultiQC, only need to be run once. Other steps, like FastQC and STAR alignment, must be run for every single sample. So, we will have to reconfigure our pipeline to run efficiently on the cluster.","title":"Running the parallelised pipeline on Imperial computing cluster"},{"location":"running_parallelised_pipeline/#set-up","text":"To run the full pipeline on the cluster, you must have setup the conda environment and downloaded this repository. The instructions to do this are available in this exemplar's top-level README.md . Note, however, that conda works slightly differently on the imperial cluster. You must activate the conda module using the following code: module load anaconda3/personal If you have never run conda before, you must initialise it like so: anaconda-setup Finally, you can install the recode_rnaseq environment: conda update conda conda env create -f environment.yml Note that to run the pipeline and to install the conda environment, you must run the commands from within the ReCoDE_rnaseq_pipeline repository. The code to setup the conda environment is also included in the script pbs/setup_pipeline.pbs , so you could just uncomment those lines and run the pipeline and install the environment in one go. The install commands in this script use mamba , which is a lot faster than conda. So, if you're having trouble installing the environment, try using mamba . You can then run the pipeline either by running the command workflows/parallelised_pipeline.sh from the cluster command line (do not use the qsub command to run this script), or by running the individual lines of code within workflows/parallelised_pipeline.sh . On the Imperial cluster, this should automatically run the pipeline for the test dataset. If you are using a cluster owned by a different institution, you may need to do some work to adapt the scripts to your scheduler. If you have any problems, please raise them as an issue .","title":"Set up"},{"location":"running_parallelised_pipeline/#adapting-the-simple-pipeline-to-run-on-the-cluster","text":"Our first step is to break the pipeline up into multiple stages. We need to separate the elements of the pipeline that should only be run once, and the parts of the pipeline that need to be run for each sample. Specifically, we need to create the results folders before we start generating results. We also need to create a STAR index before we run the alignment step. So, we create a script, pbs/setup_pipeline.pbs , which runs these stages first. While MultiQC is another step that only needs to be run once, it can't be run in this script because it has to be run last! So, the next step is to carry out the data processing steps for each sample. In workflows/simple_local_pipeline.sh , we used loops to run the QC, trimming, alignment and counting scripts. Since we are now on the cluster, we will instead leverage array jobs to run these steps for each of our samples. We create a script, pbs/parallel_samples.pbs , which is designed to process a single sample. In the workflows/simple_local_pipeline.sh script, we looped through the sample IDs, and in each iteration we assigned the sample ID to the variable s . In pbs/parallel_samples.pbs , we take all the code from within these loops, but we instead define s at the top of the script using the environment variable PBS_ARRAY_INDEX . We will later set up this script to run as an array, such that pbs/parallel_samples.pbs is run six times, once for each sample; each of the six array jobs will have a different number for the variable PBS_ARRAY_INDEX , so that each script has a different value of s . To do this, we get the list of samples names like we did in the simple pipeline, as follows: while IFS=\\= read srr; do SAMPLE_SRR+=($srr) done < data/files.txt Then, we select the Nth value of the sample list (i.e. the Nth row of data/files.txt ), in the following code: s=\"${SAMPLE_SRR[$PBS_ARRAY_INDEX - 1]}\" The script then runs the data processing steps for the corresponding sample. Once all of the samples have been processed, we can finally run MultiQC to collect the reports. This is achieved using the script pbs.multiqc.pbs . We also added the PBS parameters to the top of each of the .pbs scripts. We gave both pbs/setup_pipeline.pbs and parallel_samples.pbs multiple cores and high amounts of memory, because STAR can make use of them. MultiQC is a lot less demanding, so we gave pbs/multiqc.pbs only a single core and 4GB of RAM. These parameters might need modifications for different genomes or larger amounts of data, but for the soybean dataset they are sufficient.","title":"Adapting the simple pipeline to run on the cluster"},{"location":"running_parallelised_pipeline/#running-the-pipeline","text":"Having now split workflows/simple_local_pipeline.sh into three different stages, we can submit them to the scheduler. The code needed to do this is available in workflows/parallelised_pipeline.sh . This contains the qsub commands we need to submit each of the job to the cluster in order. The first command in this script, below, submits the setup script to the scheduler and stores its job ID to the variable sp_jid . sp_jid=\"$(qsub pbs/setup_pipeline.pbs)\" We next read the samples to be processed as a bash array, as below. We do this to calculate the number of samples to be processed, so that we know how large the array job needs to be. while IFS=\\= read srr; do SAMPLE_SRR+=($srr) done < data/files.txt The most complex command is the submission of the array job: ps_jid=\"$(qsub -W depend=afterok:\"${sp_jid}\" -J 1-\"${#SAMPLE_SRR[@]}\" pbs/parallel_samples.pbs)\" Like we did for the setup script, we submit pbs/parallel_samples.pbs to the scheduler using qsub and save its job ID to the variable ps_jid . Conveniently, all of the array jobs are connected to a single job ID, which we can store to track its progress. The argument -W depend=afterok:\"${sp_jid}\" tells the scheduler not to start the array job until the setup job is complete. Finally, the 1-\"${#SAMPLE_SRR[@]}\" argument sets the script up as an array job with array IDs from 1 to the number of samples in the SAMPLE_SRR array. Finally, we schedule the MultiQC script to run after the array job is complete, like so: mq_jid=\"$(qsub -W depend=afterok:\"${ps_jid}\" pbs/multiqc.pbs)\"","title":"Running the pipeline"},{"location":"running_parallelised_pipeline/#running-the-pipeline-for-the-full-dataset","text":"Now we can try and run the pipeline for the full soybean dataset! This is a lot bigger, but now that we are on the cluster it should be simple. The first step is to change the DATA_DIR variable in each of the scripts in the pbs/ directory from \"data/test\" to \"data/\". Then, you will need to download the data! There is a script available for doing this: data/get_data.sh . This will take some time, so you will need to run it as a PBS job. The easiest way to do this is to uncomment the code data/get_data.sh in the pbs/setup_pipeline.pbs . This way you will download the data as part of the setup - you may need to increase the lwalltime parameter so that everything runs though! Note that this script uses a tool called fastq-dump to get the data from the Sequence Read Archive. You could either try installing it using conda, or on the imperial cluster you can use the sra-toolkit module. To do this, just uncomment the line module load sra-toolkit/2.8.1 in the data/get_data.sh script. Now that you have done this, you can run the script like we did for the test data. It will take a bit longer, but not too long because we are only running six samples. The benefit of the parallelised pipeline would be even more obvious if we were to try running hundreds of samples! Once the pipeline has run, you can check the MultiQC outputs and see how they look for a real dataset. If of interest, you could try creating a job to run workflows/simple_local_pipeline.sh for the full dataset. Since we only have six samples, this will run, but it will be a lot slower than the parallelised pipeline!","title":"Running the pipeline for the full dataset"},{"location":"running_parallelised_pipeline/#limitations","text":"The parallelised pipeline alleviated some of the limitations of the simple local pipeline. Running the pipeline on the cluster gives us access to the resources we need to run compute-intensive tools like STAR. We also made use of array jobs to run each sample in parallel, making the processing a lot faster. In docs/simple_local_pipeline.md we also noted that the pipeline was prone to failure. What if a file wasn't generated for one of the samples and we never noticed? What if a file was generated but there was an error that prevented the process from finishing? These problems may have been missed by the simple pipeline. And, sadly, they may also be missed by the parallelised pipeline. If the array job fails completely, the job will finish and MultiQC won't run, because it depends on the array job. So, we might pick up major problems with the pipeline. But, there might still be an error for a particular sample that doesn't get picked up. In this case, the parallelised pipeline might be even worse than the simpler one! To make sure each stage ran correctly we would have to check each of the error files produced by each of the array jobs. We could make this pipeline more robust. For instance, we could improve the final script ( pbs/multiqc.pbs ) by adding code that checks the right files were generated for all of the samples. This would make us a lot more confident that the pipeline had run correctly. But, writing all these checks takes time that we could be using to process more data or perform downstream analysis. And, eventually we will write a pipeline and forget to check something important. In the next document, docs/nextflow_pipeline.sh , we will discuss the use of Nextflow to manage our pipeline. Nextflow will allow us to more elegantly tie our pipeline steps together, and it will make sure that every stage produces the expected outputs!","title":"Limitations"},{"location":"running_simple_local_pipeline/","text":"Running the simple pipeline Set Up Before we start attempting to process the RNA-seq data, we need to download this repository and install the relevant tools. You can get this repository by installing Git and cloning this repository from the command line, using the following code: git clone https://github.com/ImperialCollegeLondon/ReCoDE_rnaseq_pipeline.git The packages we need are listed in the file environment.yml . The easiest way to get them is to install conda , then run the following code from the command line: conda env create -f environment.yml conda activate recode_rnaseq You must be in the project directory so that conda has access to the environment.yml file. If this step takes too long to run, you could consider using mamba instead. The dataset For this tutorial, we will be using a freely available dataset that has been generated using samples from soybean plants. These data are available from the Sequence Read Archive (SRA) under the identifier SRP009826, entitled \"Comparing the response to chronic ozone of five agriculturally important legume species.\" RNA-seq was performed for soybean plants that were part of the control group (ambient O3 levels) and treatment group (elevated O3 levels). By the end of this exemplar, we will use the processed RNA-seq data to investigate the differences in gene expression between these two groups. Note that these data were used previously in the paper \" The bench scientist's guide to statistical analysis of RNA-Seq data \" and a tutorial . RNA-seq data can be quite large. Each sample might generate a few gigabytes of data. So, if you perform an experiment with hundreds of samples, your dataset might be in the order of terabytes! If you wish to try and process the full RNA-seq data, you can use the script data/get_data.sh to download the full data. However, these data may be too large to run on your laptop! For this reason, we have taken a subset of the full soybean data, which is included in this repository. These data are available in the data/test/ directory. For this notebook, we suggest using the test dataset to understand and run the simple RNA-seq pipeline. When we start using the more advanced pipelines (described in docs/parallelised_pipeline.md and docs/nextflow_pipeline.md ) you could try applying the pipeline to the full dataset using the computing cluster. The data generated by our RNA sequencing experiment is stored in the .fastq format. You can find these in the test/fastq/ directory. There is one file per sample, and each of the determined RNA sequences are stored in these files. The wikipedia article for the .fastq format includes the following example for a single sequence: @SEQ_ID GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT + !''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65 Each sequence stored in the file is described by four lines: A sequence identifier. This might include information on the type of sequencing being performed and the location of the RNA within the sequencer. The base sequence of the RNA. A + character. A quality score. There is one character for each base in line 2. . This represents the certainty of the sequencer that the base has been determined correctly. But, you might have noticed there is another directory in data/test/ . There are two files in data/test/genome/ . These files were not generated in our soybean experiment. These files actually describe the soybean genome sequence and will be useful for understanding where the RNA sequences in our experiment originated from. The two files are as follows: The .fna file (also known as a fasta) has similarities to the .fastq files. However, the genome sequence was determined in a previous experiment using genome sequencing, whereas the .fastq files using RNA sequencing to determine the RNA sequences. The .fna file contains the genome sequences, but in this case there are no quality scores. There are only two lines for each sequence; the first line gives a unique identifier while the second represents the nucleotide sequence. Each sequence in the .fna file represents one of the soybean chromosomes. The .gtf file (also known as gene transfer format) is a tab-delimited text file that contains the genome annotation. This file tells us where each gene is located within the soybean genome. This is useful, because we can use the .fna file to match the RNA sequences to the soybean genome, then we can lookup which gene this corresponds to using the .gtf annotation file. Individual data processing steps We now have some data we would like to analyse, but we need to work out how to process it. Our end goal is to count the number of RNA sequences in our samples that map to each gene in the soybean genome. Below, we will give a brief description of the analysis steps. In each step, we will use an open source tool available from conda to perform the processing. The focus on this exemplar is to show you how to create a pipeline, rather than give an in-depth explanation of the individual analysis steps; so, we suggest you look at the documentation for each tool if you are interested in how they work. For each analysis stage, we have included a bash script that runs the relevant tool in the bin/ directory. We could call the tools directly from the command line. The primary reason we put each tool into its own bash script is so that we can use this as a framework for the pipeline. So, each of the three pipeline versions we will describe in this exemplar will all use the same scripts in bin/ to run the analysis steps. Having each analysis stage packaged neatly into their own scripts will allow us to focus on the best way to orchestrate the analysis, rather than worrying about the specifics of each tool. It should also be helpful when trying to understand how each of the three pipelines are different. Quality control When describing the .fastq file format, we mentioned that the sequencer produces a quality score for each base of the RNA sequence. Low quality scores indicate that the sequencer is uncertain whether the determined bases are correct. Low quality data could cause issues for our processing and analysis steps, but we can't check the quality of each sequence individually. So, the first step in our pipeline is to use the fastqc tool to generate a summary document assessing each .fastq file. The script bin/fastqc.sh runs the fastqc command line tool for a single .fastq file. So, we will have to run this script once for each of the six samples in data/test/fastq/ . Each of the bash scripts in bin/ follow a common format. The first line contains the bash \"shebang\" #!/bin/bash which indicates that this is a bash script. This is followed by a set of comments (lines that start with the # symbol) that explain the purpose of the script, and the inputs the script takes. The bin/fastqc.sh script takes three inputs, including the input file to be analysed and where we want to write the results to. In order to pass these inputs to the bash script we can run it from the command line as so: bin/fastqc.sh \"a\" \"b\" \"c\" The string \"a\" will be accessible within the script through the variable $1 , \"b\" will be accessible through the variable $2 , and so on. We can run the script multiple times, and each time we can change the arguments to the script so it runs for each of our input samples. \"c\" represents a directory that fastqc can save temporary files to. If we were to run FastQC for one of the soybean samples, the program would generate a convenient .html report for us that creates plots describing various quality control metrics. A screenshot of one of the reports is shown below. The summary on the left lists each of the quality control metrics and uses a traffic light system to indicate issues. Not all of the metrics are relevant for our analysis; if you want to understand each metric, there is documentation available online for the FastQC program that explains them. The first metric shows a set of boxplots indicating the quality score. We mentioned these scores previously; they are available for each base in the .fastq file. FastQC converts the letter (phred) scores to numbers, where higher numbers indicate higher quality. You can see in the diagram that the quality is consistently high for each of the sequences in this .fastq file for the first ~50 bases. After this point, the quality drops off for many of the sequences. This might indicate that there are poor quality sequences that we should remove before performing further data processing steps. Sequence trimming In the FastQC report, we saw that the quality of some of the RNA sequences drops off after ~50 bases. For this reason, we might consider removing the low quality sections of the sequences. Tools such as Trim Galore can do this by trimming the low quality sequences. The FastQC report also included a section on \"Adapter Content\". As part of the sequencing process, small sequences called adapters are attached to the RNA molecules. Sometimes, these sequences are sequenced by the sequence in addition to the RNA gathered from the sample. While in this case it didn't detect any, FastQC looks for common adapter sequences that are present in the RNA sequences. In this case, trimming tools can also remove the adapter sequences before we move on to further processing steps. The trimming step is performed by the bin/trim.sh script. Notice we pass the argument --fastqc to Trim Galore. This means that, after performing trimming, the program will run FastQC on the trimmed .fastq files so that we can verify that trimming has improved the quality distribution of the sequences. Alignment In the alignment stage, we will attempt to use the alignment tool STAR to match each RNA sequence to its position in the soybean genome. Alignment is quite computationally intensive, but tools like STAR have been developed that are very efficient at the task. Part of the reason STAR is so efficient is that it performs an indexing step before starting to align sequences. This is analogous to the index at the back of a book: STAR creates an index that allows it to more efficiently find sequences in a genome. So, we first have to run the script bin/star_index.sh , which calls STAR in genomeGenerate mode. We let STAR build an index for the soybean genome to ensure that the alignment stage is a lot faster. Importantly, the indexing stage only needs to be done once, while the alignment stage must be performed for each sample. The index can be then be re-used as many times as is necessary. Having generated an index, we can run the alignment stage, using the bin/align.sh script. STAR saves the details of the alignment as a BAM (binary alignment map) file. This format is common to many different aligners and can be manipulated by lots of different bioinformatics tools. In the next stage, we will use this alignment file to count how many RNA sequences map to each gene. You might notice that in the bin/align.sh script we also use the command line application samtools to index the BAM file that was generated by STAR. This reformats the BAM file so that it is compatible with the htseq-count tool we will apply in the next stage. Counting The final stage of the data processing pipeline is to count how many RNA sequences map to each gene. In this exemplar, we use the program htseq-count to do this step, but there are lots of tools that can do this. We run this tool using the bin/count.sh script, using the BAM file as input. The output of this tool is a .counts file; each line in this file contains a gene ID, followed by a tab separator, followed by the number of RNA sequences that mapped to the gene. This file is used as the first step in the downstream analysis notebook ( downstream_analysis.md ). In this notebook, we combine the .counts file for each sample into a single matrix, which has genes as rows and samples as columns. From here, we can apply data normalisation and begin comparing our samples. Putting the steps together Having briefly discussed each stage of the pipeline, we can begin to put the stages together. We could run each stage by hand from the command line, but this would become time-consuming if we had many samples and it would be difficult to record the steps we took. Instead, we will put together a script ( workflows/simple_local_pipeline.sh ) and run our entire analysis using a single command. The following section describes each stage of the workflows/simple_local_pipeline.sh script, before showing you how to run it from the command line. conda activate recode_rnaseq So, we now have access to the tools we need to perform the analysis. We then create the DATA_DIR variable below. This is where the data we want to process is stored. The script expects to find .fastq files in the folder ${DATA_DIR}/fastq and the genome files ( .gtf and .fna ) in ${DATA_DIR}/genome . DATA_DIR=\"data/test\" We have used the test data stored within the GitHub repository to try this pipeline out. If we wanted, we could download the full dataset using the data/get_data.sh script and change DATA_DIR to \"data/\" . Feel free to try this out after running the pipeline on the test dataset, but the data might be too large to run on your laptop or home computer! This is fine, because we will process the full dataset on the cluster in the next notebook. The file data/files.txt lists the sample identifiers for the data we are using in this exemplar. The script loads the names of these samples into a bash array using the following code: while IFS=\\= read srr; do SAMPLE_SRR+=($srr) done < data/files.txt Next, like we did for DATA_DIR , we define RES_DIR , which indicates the folder where the results will be saved to. Some of the pipeline steps, like STAR, can use multiple processers to speed up the analysis. To make use of this, we can set the variable NUM_CORES to a number greater than 1. Next, we use a bash if statement, defined below. If the results folder at RES_DIR does not exist, this code creates a folder to store the results in. if [ -e \"${RES_DIR}\" ]; then echo \"Results folder already exists, previous files may be overwritten.\" else mkdir \"${RES_DIR}\" fi We then create a function called create_folder , defined below. We can call the function with a single argument, as so: create_folder(my_folder) . If the folder doesn't already exist, the function will create the folder. create_folder () { if [ ! -e \"${RES_DIR}/$1\" ]; then mkdir \"${RES_DIR}/$1\" fi } Next, we use a bash loop to run the bin/fastqc.sh step for each of our samples. In the code below, we run the fastqc script for each of the sample names in the vector SAMPLE_SRR . So, for each iteration of the loop, we give a different input .fastq file to the fastqc script, and specify a different results directory, using the variable s . After this stage, there will be a report saved to the folder 1_simple_local_pipeline_results/a_fastqc/ for each of our samples. for s in \"${SAMPLE_SRR[@]}\"; do # run fastqc on raw fastq bin/fastqc.sh \\ \"${RES_DIR}/a_fastqc\" \\ \"${DATA_DIR}/fastq/${s}.fastq.gz\" \\ \"${RES_DIR}/a_fastqc/${s}\" done After creating these reports, we use a tool we haven't discussed yet: MultiQC. MultiQC can detect reports generated by common bioinformatics tools and cleverly combines them into a single report. So, we use the bin/multiqc.sh script to generate a single report from the six FastQC reports we generated in the previous stage. When we discussed the FastQC report we focussed on the sequence quality section. In the MultiQC report, there is a section that simultaneously visualises the average sequence quality for each sample in a single plot, which should look like the plot below. Again, we can see that the average quality score begins to drop after ~50bp. The next step in the pipeline is to perform trimming. The code for this is similar to how we ran the FastQC tool. We loop through each of the samples, but this time we call bin/trim.sh . After we have performed trimming for each sample, we again run MultiQC to collect the reports after trimming. As you can see below, the average sequence quality is a lot better after trimming! Now we are more confident in the quality of our data, we can continue processing it. In the next stage of the pipeline, we run the STAR indexing script. We don't do this in a loop, because we only need to generate the index once! After the index has been created, we move on to both align the sequences for each sample and perform the counting stage. The code below again loops through each of the sample names, storing the sample name in the variable s for each iteration. Within the loop we do the following: Perform alignment using STAR for a single sample at a time. Remove the uncompressed .fastq file generated by the alignment script as it is no longer needed. Generate a .counts file using htseq-count for the sample. Check that the .counts file was successfully created. If not, something has gone wrong with the pipeline! We use a bash if statement to check the file is present in the results folder. If it is not, we use the command err to return an error. for s in \"${SAMPLE_SRR[@]}\"; do # perform alignment using STAR, providing the directory of the indexed genome bin/align.sh \\ \"${RES_DIR}/e_star_index\" \\ \"${RES_DIR}/c_trim/${s}_trimmed.fq.gz\" \\ \"${RES_DIR}/f_align/${s}\" \\ \"${NUM_CORES}\" # remove unzipped fastq rm \"${RES_DIR}/f_align/${s}.fastq\" bin/count.sh \\ \"${RES_DIR}/f_align/${s}Aligned.sortedByCoord.out.bam\" \\ \"${RES_DIR}/e_star_index/GCF_000004515.6_Glycine_max_v4.0_genomic.gtf\" \\ \"${RES_DIR}/g_count/${s}\" # check the counts have been successfully created if [ -e \"${RES_DIR}/g_count/${s}.counts\" ]; then echo \"Counts file for sample ${s} was successfully created\" else err \"Counts file for sample ${s} was not created\" fi done In the final stage of the pipeline we run MultiQC yet again. MultiQC can also assess the output of STAR and htseq-count to check that the alignment and counting stages have performed adequately. If you run the pipeline on the test dataset and check the output for STAR, you might notice that only a small proportion of the reads were actually mapped to the genome. Usually this would be a big problem! However, in this case the alignment is only poor because of the way in which the test dataset was generated. When we run the pipeline for the full soybean data, we will find that the vast majority of RNA sequences are uniquely mapped to the soybean genome. Now we have discussed how the pipeline was created, we can actually run it. If you paste the following code into the command line, the pipeline should run for the test dataset: workflows/simple_local_pipeline.sh If you have any problems running the pipeline, feel free to report them as an issue on GitHub . If you find any mistakes and know how to correct them, you could also make the modification and create a pull request . If you get an error indicating any of the scripts in the project are not executable, you can fix this with the chmod command line application. This has been a very rapid introduction to RNA-seq and its analysis, so it's normal to feel overwhelmed if this was your first exposure to the technology. After you've run the pipeline, we suggest you spend some time looking at the results of each stage and exploring the reports collected by MultiQC. There are lots of resources for learning about RNA-seq in more detail and the MultiQC report contains links to resources that explain each of the tools. However, do note that the report you generate is based on the test data and so may not be representative of a \"true\" RNA-seq dataset! If you want to explore the report generated for the full data, you can move on to the next notebook ( docs/parallelised_pipeline.md ) where we will run the pipeline for the full dataset. Alternatively, you can open the report in assets/multiqc_report.html , which contains a MultiQC report generated for the full dataset. Limitations In the next notebook ( docs/parallelised_pipeline.md ) we will move the pipeline onto the computing cluster at Imperial. This pipeline will use the same basic steps as the one described in the current document, but it will improve upon it in a few areas. The pipeline we just ran is only set up to run on a locally on a single computer. In all likelihood, your laptop or work computer has a up to 8 cores and 16GB RAM (memory). Using tools like STAR for the human genome requires at least 30GB of memory to perform the indexing step efficiently. Furthermore, ~8 cores isn't very many if we want to start running multiple samples at the same time. Currently, the pipeline is set up to apply the tools one sample at a time using for loops. While this may be fine for a small dataset that contains only six samples, this may become a problem for larger datasets! Ideally, we would like to run many samples simultaneously so that we can process our RNA-seq data in a reasonable time-frame. The parallelised pipeline will attempt to solve these problems. Using a computing cluster, such as Imperial's high performance computing cluster, will give you access to a huge number of processing cores and memory. We can run up to 50 different \"jobs\" on the computing cluster at Imperial, and each of these can have access to more cores and memory than your work computer. There are other limitations of our current pipeline that you should be aware of, but these won't necessarily be solved by the next pipeline iteration. In the current pipeline, if a single pipeline step fails the entire pipeline fails. If the pipeline has run for 30 hours and fails on sample 100, we must fix the issue and start again from scratch! Another problem is that we don't rigorously check that our pipeline is doing the right thing. For instance, if trimming fails for one of our samples and doesn't produce an output, the pipeline may keep going on without it. We might end up collecting our final .counts files at the end of the pipeline without ever realising we have lost a sample! We could put more work into this pipeline to make sure cases like these don't happen. For instance, we check at the end of the pipeline that all of the .counts files are accounted for. But, covering all of the possible errors is difficult and it would be easy to miss something. After covering the migration of the pipeline onto the computing cluster, we will discuss the use of a tool called Nextflow, which makes creating robust pipelines a lot simpler!","title":"Simple Pipeline - Running"},{"location":"running_simple_local_pipeline/#running-the-simple-pipeline","text":"","title":"Running the simple pipeline"},{"location":"running_simple_local_pipeline/#set-up","text":"Before we start attempting to process the RNA-seq data, we need to download this repository and install the relevant tools. You can get this repository by installing Git and cloning this repository from the command line, using the following code: git clone https://github.com/ImperialCollegeLondon/ReCoDE_rnaseq_pipeline.git The packages we need are listed in the file environment.yml . The easiest way to get them is to install conda , then run the following code from the command line: conda env create -f environment.yml conda activate recode_rnaseq You must be in the project directory so that conda has access to the environment.yml file. If this step takes too long to run, you could consider using mamba instead.","title":"Set Up"},{"location":"running_simple_local_pipeline/#the-dataset","text":"For this tutorial, we will be using a freely available dataset that has been generated using samples from soybean plants. These data are available from the Sequence Read Archive (SRA) under the identifier SRP009826, entitled \"Comparing the response to chronic ozone of five agriculturally important legume species.\" RNA-seq was performed for soybean plants that were part of the control group (ambient O3 levels) and treatment group (elevated O3 levels). By the end of this exemplar, we will use the processed RNA-seq data to investigate the differences in gene expression between these two groups. Note that these data were used previously in the paper \" The bench scientist's guide to statistical analysis of RNA-Seq data \" and a tutorial . RNA-seq data can be quite large. Each sample might generate a few gigabytes of data. So, if you perform an experiment with hundreds of samples, your dataset might be in the order of terabytes! If you wish to try and process the full RNA-seq data, you can use the script data/get_data.sh to download the full data. However, these data may be too large to run on your laptop! For this reason, we have taken a subset of the full soybean data, which is included in this repository. These data are available in the data/test/ directory. For this notebook, we suggest using the test dataset to understand and run the simple RNA-seq pipeline. When we start using the more advanced pipelines (described in docs/parallelised_pipeline.md and docs/nextflow_pipeline.md ) you could try applying the pipeline to the full dataset using the computing cluster. The data generated by our RNA sequencing experiment is stored in the .fastq format. You can find these in the test/fastq/ directory. There is one file per sample, and each of the determined RNA sequences are stored in these files. The wikipedia article for the .fastq format includes the following example for a single sequence: @SEQ_ID GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT + !''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65 Each sequence stored in the file is described by four lines: A sequence identifier. This might include information on the type of sequencing being performed and the location of the RNA within the sequencer. The base sequence of the RNA. A + character. A quality score. There is one character for each base in line 2. . This represents the certainty of the sequencer that the base has been determined correctly. But, you might have noticed there is another directory in data/test/ . There are two files in data/test/genome/ . These files were not generated in our soybean experiment. These files actually describe the soybean genome sequence and will be useful for understanding where the RNA sequences in our experiment originated from. The two files are as follows: The .fna file (also known as a fasta) has similarities to the .fastq files. However, the genome sequence was determined in a previous experiment using genome sequencing, whereas the .fastq files using RNA sequencing to determine the RNA sequences. The .fna file contains the genome sequences, but in this case there are no quality scores. There are only two lines for each sequence; the first line gives a unique identifier while the second represents the nucleotide sequence. Each sequence in the .fna file represents one of the soybean chromosomes. The .gtf file (also known as gene transfer format) is a tab-delimited text file that contains the genome annotation. This file tells us where each gene is located within the soybean genome. This is useful, because we can use the .fna file to match the RNA sequences to the soybean genome, then we can lookup which gene this corresponds to using the .gtf annotation file.","title":"The dataset"},{"location":"running_simple_local_pipeline/#individual-data-processing-steps","text":"We now have some data we would like to analyse, but we need to work out how to process it. Our end goal is to count the number of RNA sequences in our samples that map to each gene in the soybean genome. Below, we will give a brief description of the analysis steps. In each step, we will use an open source tool available from conda to perform the processing. The focus on this exemplar is to show you how to create a pipeline, rather than give an in-depth explanation of the individual analysis steps; so, we suggest you look at the documentation for each tool if you are interested in how they work. For each analysis stage, we have included a bash script that runs the relevant tool in the bin/ directory. We could call the tools directly from the command line. The primary reason we put each tool into its own bash script is so that we can use this as a framework for the pipeline. So, each of the three pipeline versions we will describe in this exemplar will all use the same scripts in bin/ to run the analysis steps. Having each analysis stage packaged neatly into their own scripts will allow us to focus on the best way to orchestrate the analysis, rather than worrying about the specifics of each tool. It should also be helpful when trying to understand how each of the three pipelines are different.","title":"Individual data processing steps"},{"location":"running_simple_local_pipeline/#quality-control","text":"When describing the .fastq file format, we mentioned that the sequencer produces a quality score for each base of the RNA sequence. Low quality scores indicate that the sequencer is uncertain whether the determined bases are correct. Low quality data could cause issues for our processing and analysis steps, but we can't check the quality of each sequence individually. So, the first step in our pipeline is to use the fastqc tool to generate a summary document assessing each .fastq file. The script bin/fastqc.sh runs the fastqc command line tool for a single .fastq file. So, we will have to run this script once for each of the six samples in data/test/fastq/ . Each of the bash scripts in bin/ follow a common format. The first line contains the bash \"shebang\" #!/bin/bash which indicates that this is a bash script. This is followed by a set of comments (lines that start with the # symbol) that explain the purpose of the script, and the inputs the script takes. The bin/fastqc.sh script takes three inputs, including the input file to be analysed and where we want to write the results to. In order to pass these inputs to the bash script we can run it from the command line as so: bin/fastqc.sh \"a\" \"b\" \"c\" The string \"a\" will be accessible within the script through the variable $1 , \"b\" will be accessible through the variable $2 , and so on. We can run the script multiple times, and each time we can change the arguments to the script so it runs for each of our input samples. \"c\" represents a directory that fastqc can save temporary files to. If we were to run FastQC for one of the soybean samples, the program would generate a convenient .html report for us that creates plots describing various quality control metrics. A screenshot of one of the reports is shown below. The summary on the left lists each of the quality control metrics and uses a traffic light system to indicate issues. Not all of the metrics are relevant for our analysis; if you want to understand each metric, there is documentation available online for the FastQC program that explains them. The first metric shows a set of boxplots indicating the quality score. We mentioned these scores previously; they are available for each base in the .fastq file. FastQC converts the letter (phred) scores to numbers, where higher numbers indicate higher quality. You can see in the diagram that the quality is consistently high for each of the sequences in this .fastq file for the first ~50 bases. After this point, the quality drops off for many of the sequences. This might indicate that there are poor quality sequences that we should remove before performing further data processing steps.","title":"Quality control"},{"location":"running_simple_local_pipeline/#sequence-trimming","text":"In the FastQC report, we saw that the quality of some of the RNA sequences drops off after ~50 bases. For this reason, we might consider removing the low quality sections of the sequences. Tools such as Trim Galore can do this by trimming the low quality sequences. The FastQC report also included a section on \"Adapter Content\". As part of the sequencing process, small sequences called adapters are attached to the RNA molecules. Sometimes, these sequences are sequenced by the sequence in addition to the RNA gathered from the sample. While in this case it didn't detect any, FastQC looks for common adapter sequences that are present in the RNA sequences. In this case, trimming tools can also remove the adapter sequences before we move on to further processing steps. The trimming step is performed by the bin/trim.sh script. Notice we pass the argument --fastqc to Trim Galore. This means that, after performing trimming, the program will run FastQC on the trimmed .fastq files so that we can verify that trimming has improved the quality distribution of the sequences.","title":"Sequence trimming"},{"location":"running_simple_local_pipeline/#alignment","text":"In the alignment stage, we will attempt to use the alignment tool STAR to match each RNA sequence to its position in the soybean genome. Alignment is quite computationally intensive, but tools like STAR have been developed that are very efficient at the task. Part of the reason STAR is so efficient is that it performs an indexing step before starting to align sequences. This is analogous to the index at the back of a book: STAR creates an index that allows it to more efficiently find sequences in a genome. So, we first have to run the script bin/star_index.sh , which calls STAR in genomeGenerate mode. We let STAR build an index for the soybean genome to ensure that the alignment stage is a lot faster. Importantly, the indexing stage only needs to be done once, while the alignment stage must be performed for each sample. The index can be then be re-used as many times as is necessary. Having generated an index, we can run the alignment stage, using the bin/align.sh script. STAR saves the details of the alignment as a BAM (binary alignment map) file. This format is common to many different aligners and can be manipulated by lots of different bioinformatics tools. In the next stage, we will use this alignment file to count how many RNA sequences map to each gene. You might notice that in the bin/align.sh script we also use the command line application samtools to index the BAM file that was generated by STAR. This reformats the BAM file so that it is compatible with the htseq-count tool we will apply in the next stage.","title":"Alignment"},{"location":"running_simple_local_pipeline/#counting","text":"The final stage of the data processing pipeline is to count how many RNA sequences map to each gene. In this exemplar, we use the program htseq-count to do this step, but there are lots of tools that can do this. We run this tool using the bin/count.sh script, using the BAM file as input. The output of this tool is a .counts file; each line in this file contains a gene ID, followed by a tab separator, followed by the number of RNA sequences that mapped to the gene. This file is used as the first step in the downstream analysis notebook ( downstream_analysis.md ). In this notebook, we combine the .counts file for each sample into a single matrix, which has genes as rows and samples as columns. From here, we can apply data normalisation and begin comparing our samples.","title":"Counting"},{"location":"running_simple_local_pipeline/#putting-the-steps-together","text":"Having briefly discussed each stage of the pipeline, we can begin to put the stages together. We could run each stage by hand from the command line, but this would become time-consuming if we had many samples and it would be difficult to record the steps we took. Instead, we will put together a script ( workflows/simple_local_pipeline.sh ) and run our entire analysis using a single command. The following section describes each stage of the workflows/simple_local_pipeline.sh script, before showing you how to run it from the command line. conda activate recode_rnaseq So, we now have access to the tools we need to perform the analysis. We then create the DATA_DIR variable below. This is where the data we want to process is stored. The script expects to find .fastq files in the folder ${DATA_DIR}/fastq and the genome files ( .gtf and .fna ) in ${DATA_DIR}/genome . DATA_DIR=\"data/test\" We have used the test data stored within the GitHub repository to try this pipeline out. If we wanted, we could download the full dataset using the data/get_data.sh script and change DATA_DIR to \"data/\" . Feel free to try this out after running the pipeline on the test dataset, but the data might be too large to run on your laptop or home computer! This is fine, because we will process the full dataset on the cluster in the next notebook. The file data/files.txt lists the sample identifiers for the data we are using in this exemplar. The script loads the names of these samples into a bash array using the following code: while IFS=\\= read srr; do SAMPLE_SRR+=($srr) done < data/files.txt Next, like we did for DATA_DIR , we define RES_DIR , which indicates the folder where the results will be saved to. Some of the pipeline steps, like STAR, can use multiple processers to speed up the analysis. To make use of this, we can set the variable NUM_CORES to a number greater than 1. Next, we use a bash if statement, defined below. If the results folder at RES_DIR does not exist, this code creates a folder to store the results in. if [ -e \"${RES_DIR}\" ]; then echo \"Results folder already exists, previous files may be overwritten.\" else mkdir \"${RES_DIR}\" fi We then create a function called create_folder , defined below. We can call the function with a single argument, as so: create_folder(my_folder) . If the folder doesn't already exist, the function will create the folder. create_folder () { if [ ! -e \"${RES_DIR}/$1\" ]; then mkdir \"${RES_DIR}/$1\" fi } Next, we use a bash loop to run the bin/fastqc.sh step for each of our samples. In the code below, we run the fastqc script for each of the sample names in the vector SAMPLE_SRR . So, for each iteration of the loop, we give a different input .fastq file to the fastqc script, and specify a different results directory, using the variable s . After this stage, there will be a report saved to the folder 1_simple_local_pipeline_results/a_fastqc/ for each of our samples. for s in \"${SAMPLE_SRR[@]}\"; do # run fastqc on raw fastq bin/fastqc.sh \\ \"${RES_DIR}/a_fastqc\" \\ \"${DATA_DIR}/fastq/${s}.fastq.gz\" \\ \"${RES_DIR}/a_fastqc/${s}\" done After creating these reports, we use a tool we haven't discussed yet: MultiQC. MultiQC can detect reports generated by common bioinformatics tools and cleverly combines them into a single report. So, we use the bin/multiqc.sh script to generate a single report from the six FastQC reports we generated in the previous stage. When we discussed the FastQC report we focussed on the sequence quality section. In the MultiQC report, there is a section that simultaneously visualises the average sequence quality for each sample in a single plot, which should look like the plot below. Again, we can see that the average quality score begins to drop after ~50bp. The next step in the pipeline is to perform trimming. The code for this is similar to how we ran the FastQC tool. We loop through each of the samples, but this time we call bin/trim.sh . After we have performed trimming for each sample, we again run MultiQC to collect the reports after trimming. As you can see below, the average sequence quality is a lot better after trimming! Now we are more confident in the quality of our data, we can continue processing it. In the next stage of the pipeline, we run the STAR indexing script. We don't do this in a loop, because we only need to generate the index once! After the index has been created, we move on to both align the sequences for each sample and perform the counting stage. The code below again loops through each of the sample names, storing the sample name in the variable s for each iteration. Within the loop we do the following: Perform alignment using STAR for a single sample at a time. Remove the uncompressed .fastq file generated by the alignment script as it is no longer needed. Generate a .counts file using htseq-count for the sample. Check that the .counts file was successfully created. If not, something has gone wrong with the pipeline! We use a bash if statement to check the file is present in the results folder. If it is not, we use the command err to return an error. for s in \"${SAMPLE_SRR[@]}\"; do # perform alignment using STAR, providing the directory of the indexed genome bin/align.sh \\ \"${RES_DIR}/e_star_index\" \\ \"${RES_DIR}/c_trim/${s}_trimmed.fq.gz\" \\ \"${RES_DIR}/f_align/${s}\" \\ \"${NUM_CORES}\" # remove unzipped fastq rm \"${RES_DIR}/f_align/${s}.fastq\" bin/count.sh \\ \"${RES_DIR}/f_align/${s}Aligned.sortedByCoord.out.bam\" \\ \"${RES_DIR}/e_star_index/GCF_000004515.6_Glycine_max_v4.0_genomic.gtf\" \\ \"${RES_DIR}/g_count/${s}\" # check the counts have been successfully created if [ -e \"${RES_DIR}/g_count/${s}.counts\" ]; then echo \"Counts file for sample ${s} was successfully created\" else err \"Counts file for sample ${s} was not created\" fi done In the final stage of the pipeline we run MultiQC yet again. MultiQC can also assess the output of STAR and htseq-count to check that the alignment and counting stages have performed adequately. If you run the pipeline on the test dataset and check the output for STAR, you might notice that only a small proportion of the reads were actually mapped to the genome. Usually this would be a big problem! However, in this case the alignment is only poor because of the way in which the test dataset was generated. When we run the pipeline for the full soybean data, we will find that the vast majority of RNA sequences are uniquely mapped to the soybean genome. Now we have discussed how the pipeline was created, we can actually run it. If you paste the following code into the command line, the pipeline should run for the test dataset: workflows/simple_local_pipeline.sh If you have any problems running the pipeline, feel free to report them as an issue on GitHub . If you find any mistakes and know how to correct them, you could also make the modification and create a pull request . If you get an error indicating any of the scripts in the project are not executable, you can fix this with the chmod command line application. This has been a very rapid introduction to RNA-seq and its analysis, so it's normal to feel overwhelmed if this was your first exposure to the technology. After you've run the pipeline, we suggest you spend some time looking at the results of each stage and exploring the reports collected by MultiQC. There are lots of resources for learning about RNA-seq in more detail and the MultiQC report contains links to resources that explain each of the tools. However, do note that the report you generate is based on the test data and so may not be representative of a \"true\" RNA-seq dataset! If you want to explore the report generated for the full data, you can move on to the next notebook ( docs/parallelised_pipeline.md ) where we will run the pipeline for the full dataset. Alternatively, you can open the report in assets/multiqc_report.html , which contains a MultiQC report generated for the full dataset.","title":"Putting the steps together"},{"location":"running_simple_local_pipeline/#limitations","text":"In the next notebook ( docs/parallelised_pipeline.md ) we will move the pipeline onto the computing cluster at Imperial. This pipeline will use the same basic steps as the one described in the current document, but it will improve upon it in a few areas. The pipeline we just ran is only set up to run on a locally on a single computer. In all likelihood, your laptop or work computer has a up to 8 cores and 16GB RAM (memory). Using tools like STAR for the human genome requires at least 30GB of memory to perform the indexing step efficiently. Furthermore, ~8 cores isn't very many if we want to start running multiple samples at the same time. Currently, the pipeline is set up to apply the tools one sample at a time using for loops. While this may be fine for a small dataset that contains only six samples, this may become a problem for larger datasets! Ideally, we would like to run many samples simultaneously so that we can process our RNA-seq data in a reasonable time-frame. The parallelised pipeline will attempt to solve these problems. Using a computing cluster, such as Imperial's high performance computing cluster, will give you access to a huge number of processing cores and memory. We can run up to 50 different \"jobs\" on the computing cluster at Imperial, and each of these can have access to more cores and memory than your work computer. There are other limitations of our current pipeline that you should be aware of, but these won't necessarily be solved by the next pipeline iteration. In the current pipeline, if a single pipeline step fails the entire pipeline fails. If the pipeline has run for 30 hours and fails on sample 100, we must fix the issue and start again from scratch! Another problem is that we don't rigorously check that our pipeline is doing the right thing. For instance, if trimming fails for one of our samples and doesn't produce an output, the pipeline may keep going on without it. We might end up collecting our final .counts files at the end of the pipeline without ever realising we have lost a sample! We could put more work into this pipeline to make sure cases like these don't happen. For instance, we check at the end of the pipeline that all of the .counts files are accounted for. But, covering all of the possible errors is difficult and it would be easy to miss something. After covering the migration of the pipeline onto the computing cluster, we will discuss the use of a tool called Nextflow, which makes creating robust pipelines a lot simpler!","title":"Limitations"}]}